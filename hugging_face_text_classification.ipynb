{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOjyp5sWlZX1zfyT9lE+ZV6",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banno-0720/learning_PyTorch/blob/main/hugging_face_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1 Overview\n",
        "\n",
        "Welcome to the Learn Hugging Face Text Classificaiton project!\n",
        "\n",
        "We’ll start with a text dataset, build a model to classify text samples and then share our model as a demo others can use.\n",
        "\n",
        "To do so, we’ll be using a handful of helpful open-source tools from the Hugging Face ecosystem.\n",
        "<figure style=\"text-align: center;\">\n",
        "    <!-- figtemplate -->\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-text-classification/00-project-food-not-food-overview.png\"\n",
        "     alt=\"\n",
        "Project overview image for 'Food Not Food' classification at Nutrify, a food app. The project involves building and deploying a binary text classification model to identify food-related text using Hugging Face Datasets, Transformers, and deploying with Hugging Face Hub/Spaces and Gradio. Examples include labels for 'A photo of sushi rolls on a white plate' (food), 'A serving of chicken curry in a blue bowl' (food), and 'A yellow tractor driving over a grassy hill' (not food). The process is visually depicted from data collection to model training and demo deployment.\"\n",
        "     style=\"width: 100%; max-width: 900px; height: auto;\"/>\n",
        "     <figcaption>We're going to put on our internship hats and build a food not food text classification model using tools from the Hugging Face ecosystem.</figcaption>\n",
        "</figure>\n",
        "\n"
      ],
      "metadata": {
        "id": "LT10DxJxZnhl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVje-E82evs3"
      },
      "source": [
        "## 1.1 What we're going to build\n",
        "\n",
        "We're going to be bulding a `food`/`not_food` **text classification model**.\n",
        "\n",
        "Given a piece of a text (such as an image caption), our model will be able to predict if it's about food or not.\n",
        "\n",
        "More specifically, we're going to follow the following steps:\n",
        "\n",
        "1. **[Data](https://huggingface.co/datasets/mrdbourke/learn_hf_food_not_food_image_captions): Problem defintion and dataset preparation** - Getting a dataset/setting up the problem space.\n",
        "2. **[Model](https://huggingface.co/mrdbourke/learn_hf_food_not_food_text_classifier-distilbert-base-uncased): Finding, training and evaluating a model** - Finding a text classification model suitable for our problem on Hugging Face and customizing it to our own dataset.\n",
        "3. **[Demo](https://huggingface.co/spaces/mrdbourke/learn_hf_food_not_food_text_classifier_demo): Creating a demo and put our model into the real world** - Sharing our trained model in a way others can access and use.\n",
        "\n",
        "By the end of this project, you'll have a trained model and [demo on Hugging Face](https://huggingface.co/spaces/mrdbourke/learn_hf_food_not_food_text_classifier_demo) you can share with others:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "sSkT0OZdevs4",
        "outputId": "ef12ee01-9353-461f-d294-62fc4c18f668",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 672
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "<iframe\n",
              "\tsrc=\"https://mrdbourke-learn-hf-food-not-food-text-classifier-demo.hf.space\"\n",
              "\tframeborder=\"0\"\n",
              "\twidth=\"850\"\n",
              "\theight=\"650\"\n",
              "></iframe>\n"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "from IPython.display import HTML\n",
        "\n",
        "HTML(\"\"\"\n",
        "<iframe\n",
        "\tsrc=\"https://mrdbourke-learn-hf-food-not-food-text-classifier-demo.hf.space\"\n",
        "\tframeborder=\"0\"\n",
        "\twidth=\"850\"\n",
        "\theight=\"650\"\n",
        "></iframe>\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nGilzDKJevs7"
      },
      "source": [
        "## 1.2 What is text classification?\n",
        "\n",
        "Text classification is the process of assigning a category to a piece of text.\n",
        "\n",
        "Where a category can be almost anything and a piece of text can be a word, phrase, sentence, paragraph or entire document.\n",
        "\n",
        "Example text classification problems include:\n",
        "\n",
        "| **Problem** | **Description** | **Problem Type** |\n",
        "|-----|-----|-----|\n",
        "| Spam/phishing email detection | Is an email spam or not spam? Or is it a phishing email or not? | Binary classification (one thing or another) |\n",
        "| Sentiment analysis | Is a piece of text positive, negative or neutral? Such as classifying product reviews into good/bad/neutral. | Multi-class classification (one thing from many) |\n",
        "| Language detection | What language is a piece of text written in? | Multi-class classification (one thing from many) |\n",
        "| Topic classification | What topic(s) does a news article belong to? | Multi-label classification (one or more things from many) |\n",
        "| Hate speech detection | Is a comment hateful or not hateful? | Binary classification (one thing or another) |\n",
        "| Product categorization | What categories does a product belong to? | Multi-label classification (one or more things from many) |\n",
        "| Business email classification | Which category should this email go to? | Multi-class classification (one thing from many) |\n",
        "\n",
        "Text classification is a very common problem in many business settings.\n",
        "\n",
        "For example, a project I've worked on previously as a machine learning engineer was building a text classification model to classify different insurance claims into `claimant_at_fault`/`claimant_not_at_fault` for a large insurance company.\n",
        "\n",
        "<figure style=\"text-align: center;\">\n",
        "    <!-- figtemplate -->\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-text-classification/02-example-text-classification-workflow.png\"\n",
        "     alt=\"Diagram showing an example text classification workflow for insurance claims. It includes customer-submitted text such as 'Someone crashed into my car' and 'I accidentally crashed into someone's letterbox'. These texts are processed through an insurance app by Lime Insurance Co. and input into a model that determines fault based on the text. The classification outputs are 'Not at Fault' and 'At Fault', allowing claims to be forwarded to the appropriate department. The workflow is broken down into steps: Insurance claims, Insurance app, Model, and Classification outputs.\"\n",
        "     style=\"width: 100%; max-width: 900px; height: auto;\"/>\n",
        "     <figcaption>An example text classification problem I once worked on to classify insurance claim texts into at fault or not fault. This result of the model would send the claim to a different department in the insurance company.</figcaption>\n",
        "</figure>\n",
        "\n",
        "Speaking of models, there are several different kinds of models you can use for text classification.\n",
        "\n",
        "And each will have its pros and cons depending on the problem you're working on.\n",
        "\n",
        "Example text classification models include:\n",
        "\n",
        "| **Model** | **Description** | **Pros** | **Cons** |\n",
        "|-----|-----|-----|-----|\n",
        "| Rule-based | Uses a set of rules to classify text (e.g. if text contains \"sad\" -> sentiment = low) | Simple, easy to understand | Requires manual creation of rules |\n",
        "| [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) | Counts the frequency of words in a piece of text | Simple, easy to understand | Doesn't capture word order |\n",
        "| [TF-IDF](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) | Weighs the importance of words in a piece of text | Simple, easy to understand | Doesn't capture word order |\n",
        "| Deep learning-based models | Uses neural networks to learn patterns in text | Can learn complex patterns at scale | Can require large amounts of data/compute power to run, not as easy to understand (can be hard to debug) |\n",
        "\n",
        "For our project, we're going to go with a deep learning model.\n",
        "\n",
        "Why?\n",
        "\n",
        "Because Hugging Face helps us do so.\n",
        "\n",
        "And in most cases, with a quality dataset, a deep learning model will often perform better than a rule-based or other model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pXyQ3z3wevs8"
      },
      "source": [
        "## 1.3 Why train your own text classification models?\n",
        "\n",
        "You can customize **pre-trained models** for text classification as well as API-powered models and LLMs such as GPT, Gemini, Claude or Mistral.\n",
        "\n",
        "Depending on your requirements, there are several pros and cons for using your own model versus using an API.\n",
        "\n",
        "Training/fine-tuning your own model:\n",
        "\n",
        "| Pros | Cons |\n",
        "| :----- | :----- |\n",
        "| **Control:** Full control over model lifecycle. | Can be complex to get setup. |\n",
        "| No usage limits (aside from compute constraints). | Requires dedicated compute resources for training/inference. |\n",
        "| Can train once and deploy everywhere/whenever you want (for example, Tesla deploying a model to all self-driving cars). | Requires maintenance over time to ensure performance remains up to par. |\n",
        "| **Privacy:** Data can be kept in-house/app and doesn’t need to go to a third party. | Can require longer development cycles compared to using existing APIs. |\n",
        "| **Speed:** Customizing a small model for a specific use case often means it runs much faster. | |\n",
        "\n",
        "Using a pre-built model API (e.g. GPT, Gemini, Claude, Mistral):\n",
        "\n",
        "| Pros | Cons |\n",
        "| :----- | :----- |\n",
        "| **Ease of use:** often can be setup within a few lines of code. | If the model API goes down, your service goes down. |\n",
        "| No maintenance of compute resources. | Data is required to be sent to a third-party for processing. |\n",
        "| Access to the most advanced models. | The API may have usage limits per day/time period. |\n",
        "| Can scale if usage increases. | Can be much slower than using dedicated models due to requiring an API call. |\n",
        "\n",
        "For this project, we're going to focus on fine-tuning our own model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0utdPghNevs9"
      },
      "source": [
        "## 1.4 Workflow we're going to follow\n",
        "\n",
        "Our motto is *data, model, demo!*\n",
        "\n",
        "So we're going to follow the rough workflow of:\n",
        "\n",
        "1. Create and preprocess data.\n",
        "2. Define the model we'd like use with [`transformers.AutoModelForSequenceClassification`](https://huggingface.co/docs/transformers/en/model_doc/auto#transformers.AutoModelForSequenceClassification) (or another similar model class).\n",
        "3. Define training arguments (these are hyperparameters for our model) with [`transformers.TrainingArguments`](https://huggingface.co/docs/transformers/en/main_classes/trainer#transformers.TrainingArguments).\n",
        "4. Pass `TrainingArguments` from 3 and target datasets to an instance of [`transformers.Trainer`](https://huggingface.co/docs/transformers/en/main_classes/trainer).\n",
        "5. Train the model by calling [`Trainer.train()`](https://huggingface.co/docs/transformers/v4.40.2/en/main_classes/trainer#transformers.Trainer.train).\n",
        "6. Save the model (to our local machine or to the Hugging Face Hub).\n",
        "7. Evaluate the trained model by making and inspecting predctions on the test data.\n",
        "8. Turn the model into a shareable demo.\n",
        "\n",
        "I say rough because machine learning projects are often non-linear in nature.\n",
        "\n",
        "As in, because machine learning projects involve many experiments, they can kind of be all over the place.\n",
        "\n",
        "But this worfklow will give us some good guidelines to follow.\n",
        "\n",
        "<figure style=\"text-align: center; display: inline-block;\">\n",
        "    <!-- figtemplate -->\n",
        "    <img src=\"https://huggingface.co/datasets/mrdbourke/learn-hf-images/resolve/main/learn-hf-text-classification/01-hugging-face-workflow.png\"\n",
        "     alt=\"The diagram shows the Hugging Face model development workflow, which includes the following steps: start with an idea or problem, get data ready (turn into tensors/create data splits), pick a pretrained model (to suit your problem), train/fine-tune the model on your custom data, evaluate the model, improve through experimentation, save and upload the fine-tuned model to the Hugging Face Hub, and turn your model into a shareable demo. Tools used in this workflow are Datasets/Tokenizers, Transformers/PEFT/Accelerate/timm, Hub/Spaces/Gradio, and Evaluate.\"\n",
        "     style=\"width: 100%; max-width: 900px; height: auto;\"/>\n",
        "     <figcaption style=\"width: 100%; box-sizing: border-box;\">A general Hugging Face workflow from idea to shared model and demo using tools from the Hugging Face ecosystem. You'll notice some of the steps don't match with our workflow outline above. This is because the text-based workflow outline above breaks some of the steps down for educational purposes. These kind of workflows are not set in stone and are more of guide than specific directions. See information on each of the tools in the <a href=\"https://huggingface.co\">Hugging Face documentation</a>.</figcaption>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oypw_hPeO2hb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}