{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOltDqMWlgFEzt2zsvWNG8R",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/banno-0720/learning_PyTorch/blob/main/2_pytorch_workflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PyTorch Workflow\n",
        "\n",
        "Let's explore an example PyTorch end-to-end workflow\n",
        "\n",
        "Resources:\n",
        "* Ground truth notebook - https://github.com/mrdbourke/pytorch-deep-learning/blob/main/01_pytorch_workflow.ipynb\n",
        "* Book version of notebook - https://www.learnpytorch.io/01_pytorch_workflow/\n",
        "* Ask a question - https://github.com/mrdbourke/pytorch-deep-learning/discussions\n",
        "* Exercise - https://www.learnpytorch.io/01_pytorch_workflow/#exercises"
      ],
      "metadata": {
        "id": "J3Ky5PLLlBNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "what_were_covering = { 1: \"data (prepare and load)\",\n",
        "                        2: \"build model\",\n",
        "                        3: \"fitting the model to data (training)\",\n",
        "                        4: \"making predictions and evaluating a model (inference)\",\n",
        "                        5: \"saving and loading a model\",\n",
        "                        6: \"putting it all together\"\n",
        "}\n",
        "\n",
        "what_were_covering"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmlBT6Okm8gT",
        "outputId": "59fe30b5-8a2a-409c-bc8b-5dc28c513eb0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{1: 'data (prepare and load)',\n",
              " 2: 'build model',\n",
              " 3: 'fitting the model to data (training)',\n",
              " 4: 'making predictions and evaluating a model (inference)',\n",
              " 5: 'saving and loading a model',\n",
              " 6: 'putting it all together'}"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn ## nn contains all of PyTorch's building blocks for neural networks\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "TE2QaW6xm9eg",
        "outputId": "ba268427-f702-476d-d68d-c2850e720ae3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.4.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Data (preparing and loading)\n",
        "\n",
        "Data can be almost anything... in machine learning:\n",
        "\n",
        "* Excel spreadsheet\n",
        "* Images of any kind\n",
        "* Videos (Youtube has lots of data...)\n",
        "* Audio like songs or podcasts\n",
        "* DNA\n",
        "* Text\n",
        "\n",
        "Machine learning is a game of two parts:\n",
        "\n",
        "1. Get data into a numerical representation.\n",
        "2. Build a model to learn patterns in that numerical representation.\n",
        "\n",
        "To showcase this, let's create some *known* data using linear regression formula.\n",
        "\n",
        "We'll use a linear regression formula to make a straight line with *known* parameters."
      ],
      "metadata": {
        "id": "msiPUDWRoBxt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "weight = 0.7\n",
        "bias = 0.3\n",
        "\n",
        "start = 0\n",
        "end = 1\n",
        "step = 0.02\n",
        "X = torch.arange(start, end, step).unsqueeze(dim=1)\n",
        "y = weight * X + bias\n",
        "\n",
        "X[:10], y[:10]"
      ],
      "metadata": {
        "id": "qyrRy1M7pCJ5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8755f0e7-da10-4334-8eb3-5f0cc388adc1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0000],\n",
              "         [0.0200],\n",
              "         [0.0400],\n",
              "         [0.0600],\n",
              "         [0.0800],\n",
              "         [0.1000],\n",
              "         [0.1200],\n",
              "         [0.1400],\n",
              "         [0.1600],\n",
              "         [0.1800]]),\n",
              " tensor([[0.3000],\n",
              "         [0.3140],\n",
              "         [0.3280],\n",
              "         [0.3420],\n",
              "         [0.3560],\n",
              "         [0.3700],\n",
              "         [0.3840],\n",
              "         [0.3980],\n",
              "         [0.4120],\n",
              "         [0.4260]]))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X), len(y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALok5cnpZ4ts",
        "outputId": "401b05d2-61af-4b6b-c531-67e73410a0d7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 50)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting data into traing and test sets (one of the most important concepts in machine learning in genral)"
      ],
      "metadata": {
        "id": "Gig2R5TgaBuJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a train/test split\n",
        "train_split = int(0.8 * len(X)) # 80%\n",
        "X_train, y_train = X[:train_split], y[:train_split]\n",
        "X_test, y_test = X[train_split:], y[train_split:]\n",
        "\n",
        "len(X_train), len(y_train), len(X_test), len(y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l6hV6FZaa4lW",
        "outputId": "d41f3e4a-a7db-41c5-ff9e-e5f714cf6887"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(40, 40, 10, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "How might we better visualize our data?\n",
        "\n",
        "This is where the data explorer's motto comes in!\n",
        "\n",
        "\"Visualize,visualize,visualize!\""
      ],
      "metadata": {
        "id": "jTQ5xg4ReIuM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uymbGaEpdvEZ",
        "outputId": "18fcbd14-9578-496d-f245-88a48500e5b5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.0000],\n",
              "         [0.0200],\n",
              "         [0.0400],\n",
              "         [0.0600],\n",
              "         [0.0800],\n",
              "         [0.1000],\n",
              "         [0.1200],\n",
              "         [0.1400],\n",
              "         [0.1600],\n",
              "         [0.1800],\n",
              "         [0.2000],\n",
              "         [0.2200],\n",
              "         [0.2400],\n",
              "         [0.2600],\n",
              "         [0.2800],\n",
              "         [0.3000],\n",
              "         [0.3200],\n",
              "         [0.3400],\n",
              "         [0.3600],\n",
              "         [0.3800],\n",
              "         [0.4000],\n",
              "         [0.4200],\n",
              "         [0.4400],\n",
              "         [0.4600],\n",
              "         [0.4800],\n",
              "         [0.5000],\n",
              "         [0.5200],\n",
              "         [0.5400],\n",
              "         [0.5600],\n",
              "         [0.5800],\n",
              "         [0.6000],\n",
              "         [0.6200],\n",
              "         [0.6400],\n",
              "         [0.6600],\n",
              "         [0.6800],\n",
              "         [0.7000],\n",
              "         [0.7200],\n",
              "         [0.7400],\n",
              "         [0.7600],\n",
              "         [0.7800]]),\n",
              " tensor([[0.3000],\n",
              "         [0.3140],\n",
              "         [0.3280],\n",
              "         [0.3420],\n",
              "         [0.3560],\n",
              "         [0.3700],\n",
              "         [0.3840],\n",
              "         [0.3980],\n",
              "         [0.4120],\n",
              "         [0.4260],\n",
              "         [0.4400],\n",
              "         [0.4540],\n",
              "         [0.4680],\n",
              "         [0.4820],\n",
              "         [0.4960],\n",
              "         [0.5100],\n",
              "         [0.5240],\n",
              "         [0.5380],\n",
              "         [0.5520],\n",
              "         [0.5660],\n",
              "         [0.5800],\n",
              "         [0.5940],\n",
              "         [0.6080],\n",
              "         [0.6220],\n",
              "         [0.6360],\n",
              "         [0.6500],\n",
              "         [0.6640],\n",
              "         [0.6780],\n",
              "         [0.6920],\n",
              "         [0.7060],\n",
              "         [0.7200],\n",
              "         [0.7340],\n",
              "         [0.7480],\n",
              "         [0.7620],\n",
              "         [0.7760],\n",
              "         [0.7900],\n",
              "         [0.8040],\n",
              "         [0.8180],\n",
              "         [0.8320],\n",
              "         [0.8460]]))"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(train_data=X_train,\n",
        "                   train_labels=y_train,\n",
        "                   test_data=X_test,\n",
        "                   test_labels=y_test,\n",
        "                   predictions=None):\n",
        "    \"\"\"\n",
        "    Plot training data, test data and compares predictions.\n",
        "    \"\"\"\n",
        "    plt.figure(figsize=(10,7))\n",
        "    plt.scatter(train_data, train_labels, c=\"b\", s=4, label=\"Training data\")\n",
        "    plt.scatter(test_data, test_labels, c=\"g\", s=4, label=\"Testing data\")\n",
        "    if predictions is not None:\n",
        "        plt.scatter(test_data, predictions, c=\"r\", s=4, label=\"Predictions\")\n",
        "    plt.legend(prop={\"size\": 14})\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "crrgIlrwewbH"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "34iCrNsRfdFy",
        "outputId": "3191565a-4894-4866-a765-fe5bf87ba2a0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Build model\n",
        "\n",
        "Our first PyTorch model!\n",
        "\n",
        "Because we're going to be building classes throught the course, I'd recommend getting familiar with OOP in Python, to do so you can use the following resource from Real Python: https://realpython.com/python3-object-oriented-programming/\n",
        "\n",
        "What our model does:\n",
        "* Start with random values (weights & bias)\n",
        "* Look at training data and adjust the random values to better represent (or get closer to) the ideal values (the weight & bias values we used to create the data)\n",
        "\n",
        "How does it do so?\n",
        "\n",
        "Through two main algorithms:\n",
        "1. Gradient descent - https://youtu.be/IHZwWFHWa-w?si=tbSD1Z82aykI9xgn\n",
        "2. Backpropagation - https://youtu.be/Ilg3gGewQ5U?si=0FQ8xK7QWD8pIjWW"
      ],
      "metadata": {
        "id": "9xsLya0KLJHa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LinearRegressionModel(nn.Module): # <- almost everything in PyTorch inherits from nn.Module\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "      self.weights = nn.Parameter(torch.randn(1,\n",
        "                                              requires_grad=True,\n",
        "                                              dtype=torch.float))\n",
        "      self.bias = nn.Parameter(torch.randn(1,\n",
        "                                          requires_grad=True,\n",
        "                                          dtype=torch.float))\n",
        "\n",
        "    # Forward method to define the computation in the model\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor: # <- \"x\" is the input data\n",
        "        return self.weights * x + self.bias # this is the linear regression formula"
      ],
      "metadata": {
        "id": "4P7r54eHMc6d"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PyTorch model building essentials\n",
        "\n",
        "* torch.nn - contains all of the building blocks for computational graphs (a neural network can be considered a computational graph)\n",
        "* torch.nn.Parameter - what parameters should our model try and learn, often a PyTorch layer from torch.nn will set these for us\n",
        "* torch.nn.Module - The base class for all neural network modules, if you subclass it, you should overwrite forward()\n",
        "* torch.optim - this where the optimizers in PyTorch live, they will help with gradient descent\n",
        "* def forward() - All nn.Module subclasses require you to overwrite forward(), this method defines what happens in the forward computation\n",
        "\n",
        "PyTorch essenetial modules cheatsheet - https://pytorch.org/tutorials/beginner/ptcheat.html"
      ],
      "metadata": {
        "id": "UUsk4CqW5JeV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking the contents of our PyTorch model\n",
        "\n",
        "Now we've created our model, let's check the contents\n",
        "\n",
        "So we can check our model parameters or what's inside our model using `.parameters()` and `.state_dict()`"
      ],
      "metadata": {
        "id": "RlT73MFi6U7M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "\n",
        "model_0 = LinearRegressionModel()\n",
        "\n",
        "list(model_0.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lPAsqQON-gnM",
        "outputId": "f92b0596-ed3e-45e9-cb22-d72d9b48099f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# List named parameters\n",
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WsRgGJed-zFO",
        "outputId": "8724cf0e-bf98-4db7-cfe3-23bf362d4f0c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight, bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "olzBJg-6_mCw",
        "outputId": "6369a94d-56d2-4692-ad72-d1e5e37acdce"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7, 0.3)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Making predictions using `torch.inference_mode()`\n",
        "\n",
        "To check our model's predictive power, let's see how well it predicts `y_test` based on `X_test` .\n",
        "\n",
        "When we pass data through our model, it's going to run through `forward()` method."
      ],
      "metadata": {
        "id": "a2SwZCOL_wZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_test, y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngxo-KXkBDgB",
        "outputId": "ad51ff6f-dc44-4f86-a083-70b34ce5a2f9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[0.8000],\n",
              "         [0.8200],\n",
              "         [0.8400],\n",
              "         [0.8600],\n",
              "         [0.8800],\n",
              "         [0.9000],\n",
              "         [0.9200],\n",
              "         [0.9400],\n",
              "         [0.9600],\n",
              "         [0.9800]]),\n",
              " tensor([[0.8600],\n",
              "         [0.8740],\n",
              "         [0.8880],\n",
              "         [0.9020],\n",
              "         [0.9160],\n",
              "         [0.9300],\n",
              "         [0.9440],\n",
              "         [0.9580],\n",
              "         [0.9720],\n",
              "         [0.9860]]))"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_preds = model_0(X_test)\n",
        "y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk-FuHooDKtl",
        "outputId": "48f7a690-e5fd-440a-d6da-d080f314f9e6"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3982],\n",
              "        [0.4049],\n",
              "        [0.4116],\n",
              "        [0.4184],\n",
              "        [0.4251],\n",
              "        [0.4318],\n",
              "        [0.4386],\n",
              "        [0.4453],\n",
              "        [0.4520],\n",
              "        [0.4588]], grad_fn=<AddBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make predictions with model\n",
        "with torch.inference_mode():\n",
        "  y_preds = model_0(X_test)\n",
        "\n",
        "# You can also do something similar with torch.no_grad(), however, inference_mode() is preferred\n",
        "# with torch.no_grad():\n",
        "#  y_preds = model_0(X_test)\n",
        "\n",
        "y_preds"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jRWncydqBIcr",
        "outputId": "84176e0f-5e68-4937-cf66-ec7a39ae2605"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3982],\n",
              "        [0.4049],\n",
              "        [0.4116],\n",
              "        [0.4184],\n",
              "        [0.4251],\n",
              "        [0.4318],\n",
              "        [0.4386],\n",
              "        [0.4453],\n",
              "        [0.4520],\n",
              "        [0.4588]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "See more on inference mode here -- https://x.com/PyTorch/status/1437838231505096708"
      ],
      "metadata": {
        "id": "yzWm34ZdRigb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_test"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LFC4xhHsBceJ",
        "outputId": "90c38442-c079-44d8-f02e-0f3b53aaf88e"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.8600],\n",
              "        [0.8740],\n",
              "        [0.8880],\n",
              "        [0.9020],\n",
              "        [0.9160],\n",
              "        [0.9300],\n",
              "        [0.9440],\n",
              "        [0.9580],\n",
              "        [0.9720],\n",
              "        [0.9860]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plot_predictions(predictions=y_preds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "id": "CHUwX_NqCp4X",
        "outputId": "d08ba48e-5eb4-41e7-96c8-44d392689740"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Train model\n",
        "\n",
        "The whole idea of training is for a model to move from some *unknown* parameters (these may be random) to some *known* parameters.\n",
        "\n",
        "Or in other words from a poor representation of the data to a better representation of the data.\n",
        "\n",
        "One way to measure how poor or how wrong your models predictions are is to use a loss function.\n",
        "\n",
        "* Note: Loss function may also be called cost function or criterion in different areas. For our case, we're going to refer to it as a loss function.\n",
        "\n",
        "Things we need to train:\n",
        "\n",
        "* **Loss function:** A function to measure how wrong your model's predictions are to the ideal outputs, lower is better.\n",
        "* **Optimizer:** Takes into account the loss of a model and adjusts the model's parameters (e.g. weight & bias in our case) to improve the loss function - https://pytorch.org/docs/stable/optim.html\n",
        "\n",
        "Inside the optimizer you'll often have to set two parameters:\n",
        "* `params` - the model parameters you'd like to optimize, for example `params=model_0.parameters()`\n",
        "* `lr` (learning rate) - the learning rate is a hyperparameter that defines how big/small the optimizer changes the parameters with each step(a small `lr` results in small changes, a large `lr` results in large changes)\n",
        "\n",
        "And specifically for PyTorch, we need:\n",
        "* A training loop\n",
        "* A testing loop"
      ],
      "metadata": {
        "id": "97BCYSxmCzZS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list(model_0.parameters())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTPhM1v9TQ0T",
        "outputId": "8dd2afdb-80f1-4d02-c64a-f45560552d82"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Parameter containing:\n",
              " tensor([0.3367], requires_grad=True),\n",
              " Parameter containing:\n",
              " tensor([0.1288], requires_grad=True)]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check out our model's parameters (a parameter is a value that the model sets itself)\n",
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efRUsfftUo0x",
        "outputId": "2a8e576b-82bf-4a70-e7df-8a3655358082"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3367])), ('bias', tensor([0.1288]))])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup a Loss function\n",
        "loss_fn = nn.L1Loss()\n",
        "# MAE_loss = torch.mean(torch.abs(y_pred-y_test))\n",
        "\n",
        "# Setup an optimizer (Stochastic Gradient Descent)\n",
        "optimizer = torch.optim.SGD(params=model_0.parameters(),\n",
        "                            lr=0.01) # lr = learning rate = possibly the most important hyperparameter you can set"
      ],
      "metadata": {
        "id": "qYsY_yJPUsRo"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Q:** Which loss function and optimizer should I use?\n",
        "\n",
        "**A:** This will be problem specific. But with experience, you'll get an idea of what works and what doesn't with your particular problem set.\n",
        "\n",
        "For example, for a regression problem(like ours), a loss function of `nn.L1Loss() and an optimizer of `torch.optim.SGD()` will suffice.\n",
        "\n",
        "But for a classification problem like classifying whether a photo is of a dog or a cat, you'll likely want to use a loss function of `nn.BCELoss()` (binary cross entropy loss)."
      ],
      "metadata": {
        "id": "ILo95okikKJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building a training loop (and a testing loop) in PyTorch\n",
        "\n",
        "A couple of things we need in a training loop:\n",
        "0. Loop through the data\n",
        "1. Forward pass (this involves data moving through our model's `forward()` fucntions) to make predictions on data - also called forward propagation\n",
        "2. Calculate the loss (compare forward pass predictions to ground truth labels)\n",
        "3. Optimizer zero grad\n",
        "4. Loss backward - move backwards through the network to calculate the gradients of each of the parameters of our model with respect to the loss (**backpropagation**)\n",
        "5. Optimizer step - use the optimizer to adjust our model's parameters to try and improve the loss (**gradient descent**)"
      ],
      "metadata": {
        "id": "nvVy2vukmwAE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(42)\n",
        "# An epoch is one loop through the data...(this is a hyperparameter because we've set it ourselves)\n",
        "epochs = 100\n",
        "\n",
        "### Training\n",
        "# 0. Loop through the data\n",
        "for epoch in range(epochs):\n",
        "  # Set the model to training mode\n",
        "  model_0.train() # train mode in PyTorch sets all parameters that require gradients to require_gradients=True\n",
        "\n",
        "  # 1. Forward pass (this involves data moving through our model's forward\n",
        "  y_pred = model_0(X_train)\n",
        "\n",
        "  # 2. Calculate the loss\n",
        "  loss = loss_fn(y_pred, y_train)\n",
        "  print(f\"Loss: {loss}\")\n",
        "\n",
        "  # 3. Optimizer zero grad\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  # 4. Loss backward (perform backpropagation on the loss with respect to the parameters of the model)\n",
        "  loss.backward()\n",
        "\n",
        "  # 5. Optimizer step (perform gradient descent)\n",
        "  optimizer.step() # by default how the optimizer changes will accumulate the loop so... we have to zero them above in step 3 for the next iteration of the loop\n",
        "\n",
        "  # Print out what's happening\n",
        "  print(f\"\\nEpoch: {epoch} | Loss: {loss}\")\n",
        "  print(f\"Model weights: {model_0.state_dict()['weights']}\\nModel bias: {model_0.state_dict()['bias']}\")\n",
        "  print(f\"Prediction before backprop: {y_pred[:10]}\\nGround truth: {y_train[:10]}\")\n",
        "  print(f\"Prediction after backprop: {model_0(X_train)[:10]}\")\n",
        "  print(f\"Model weights grad: {model_0.state_dict()['weights'].grad}\\nModel bias grad: {model_0.state_dict()['bias'].grad}\")\n",
        "  print(\"-\"*50)\n",
        "\n",
        "# model_0.eval() # turns off gradient tracking\n"
      ],
      "metadata": {
        "id": "UKNF4oRcqOiz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fecfeb66-4254-4dc3-ce2c-8e72ce1c435e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.1515873372554779\n",
            "\n",
            "Epoch: 0 | Loss: 0.1515873372554779\n",
            "Model weights: tensor([0.3952])\n",
            "Model bias: tensor([0.2788])\n",
            "Prediction before backprop: tensor([[0.2688],\n",
            "        [0.2766],\n",
            "        [0.2845],\n",
            "        [0.2923],\n",
            "        [0.3001],\n",
            "        [0.3079],\n",
            "        [0.3158],\n",
            "        [0.3236],\n",
            "        [0.3314],\n",
            "        [0.3392]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.2788],\n",
            "        [0.2867],\n",
            "        [0.2946],\n",
            "        [0.3025],\n",
            "        [0.3104],\n",
            "        [0.3183],\n",
            "        [0.3262],\n",
            "        [0.3341],\n",
            "        [0.3420],\n",
            "        [0.3499]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.14006635546684265\n",
            "\n",
            "Epoch: 1 | Loss: 0.14006635546684265\n",
            "Model weights: tensor([0.3991])\n",
            "Model bias: tensor([0.2888])\n",
            "Prediction before backprop: tensor([[0.2788],\n",
            "        [0.2867],\n",
            "        [0.2946],\n",
            "        [0.3025],\n",
            "        [0.3104],\n",
            "        [0.3183],\n",
            "        [0.3262],\n",
            "        [0.3341],\n",
            "        [0.3420],\n",
            "        [0.3499]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.2888],\n",
            "        [0.2968],\n",
            "        [0.3048],\n",
            "        [0.3128],\n",
            "        [0.3207],\n",
            "        [0.3287],\n",
            "        [0.3367],\n",
            "        [0.3447],\n",
            "        [0.3527],\n",
            "        [0.3606]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.1285453587770462\n",
            "\n",
            "Epoch: 2 | Loss: 0.1285453587770462\n",
            "Model weights: tensor([0.4030])\n",
            "Model bias: tensor([0.2988])\n",
            "Prediction before backprop: tensor([[0.2888],\n",
            "        [0.2968],\n",
            "        [0.3048],\n",
            "        [0.3128],\n",
            "        [0.3207],\n",
            "        [0.3287],\n",
            "        [0.3367],\n",
            "        [0.3447],\n",
            "        [0.3527],\n",
            "        [0.3606]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.2988],\n",
            "        [0.3069],\n",
            "        [0.3149],\n",
            "        [0.3230],\n",
            "        [0.3310],\n",
            "        [0.3391],\n",
            "        [0.3472],\n",
            "        [0.3552],\n",
            "        [0.3633],\n",
            "        [0.3713]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.11702437698841095\n",
            "\n",
            "Epoch: 3 | Loss: 0.11702437698841095\n",
            "Model weights: tensor([0.4069])\n",
            "Model bias: tensor([0.3088])\n",
            "Prediction before backprop: tensor([[0.2988],\n",
            "        [0.3069],\n",
            "        [0.3149],\n",
            "        [0.3230],\n",
            "        [0.3310],\n",
            "        [0.3391],\n",
            "        [0.3472],\n",
            "        [0.3552],\n",
            "        [0.3633],\n",
            "        [0.3713]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3088],\n",
            "        [0.3169],\n",
            "        [0.3251],\n",
            "        [0.3332],\n",
            "        [0.3414],\n",
            "        [0.3495],\n",
            "        [0.3576],\n",
            "        [0.3658],\n",
            "        [0.3739],\n",
            "        [0.3820]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.1060912236571312\n",
            "\n",
            "Epoch: 4 | Loss: 0.1060912236571312\n",
            "Model weights: tensor([0.4108])\n",
            "Model bias: tensor([0.3178])\n",
            "Prediction before backprop: tensor([[0.3088],\n",
            "        [0.3169],\n",
            "        [0.3251],\n",
            "        [0.3332],\n",
            "        [0.3414],\n",
            "        [0.3495],\n",
            "        [0.3576],\n",
            "        [0.3658],\n",
            "        [0.3739],\n",
            "        [0.3820]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3178],\n",
            "        [0.3260],\n",
            "        [0.3342],\n",
            "        [0.3425],\n",
            "        [0.3507],\n",
            "        [0.3589],\n",
            "        [0.3671],\n",
            "        [0.3753],\n",
            "        [0.3835],\n",
            "        [0.3917]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.09681284427642822\n",
            "\n",
            "Epoch: 5 | Loss: 0.09681284427642822\n",
            "Model weights: tensor([0.4146])\n",
            "Model bias: tensor([0.3258])\n",
            "Prediction before backprop: tensor([[0.3178],\n",
            "        [0.3260],\n",
            "        [0.3342],\n",
            "        [0.3425],\n",
            "        [0.3507],\n",
            "        [0.3589],\n",
            "        [0.3671],\n",
            "        [0.3753],\n",
            "        [0.3835],\n",
            "        [0.3917]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3258],\n",
            "        [0.3341],\n",
            "        [0.3424],\n",
            "        [0.3507],\n",
            "        [0.3590],\n",
            "        [0.3673],\n",
            "        [0.3756],\n",
            "        [0.3839],\n",
            "        [0.3921],\n",
            "        [0.4004]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.08908725529909134\n",
            "\n",
            "Epoch: 6 | Loss: 0.08908725529909134\n",
            "Model weights: tensor([0.4184])\n",
            "Model bias: tensor([0.3333])\n",
            "Prediction before backprop: tensor([[0.3258],\n",
            "        [0.3341],\n",
            "        [0.3424],\n",
            "        [0.3507],\n",
            "        [0.3590],\n",
            "        [0.3673],\n",
            "        [0.3756],\n",
            "        [0.3839],\n",
            "        [0.3921],\n",
            "        [0.4004]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3333],\n",
            "        [0.3417],\n",
            "        [0.3500],\n",
            "        [0.3584],\n",
            "        [0.3668],\n",
            "        [0.3752],\n",
            "        [0.3835],\n",
            "        [0.3919],\n",
            "        [0.4003],\n",
            "        [0.4086]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.08227583020925522\n",
            "\n",
            "Epoch: 7 | Loss: 0.08227583020925522\n",
            "Model weights: tensor([0.4222])\n",
            "Model bias: tensor([0.3403])\n",
            "Prediction before backprop: tensor([[0.3333],\n",
            "        [0.3417],\n",
            "        [0.3500],\n",
            "        [0.3584],\n",
            "        [0.3668],\n",
            "        [0.3752],\n",
            "        [0.3835],\n",
            "        [0.3919],\n",
            "        [0.4003],\n",
            "        [0.4086]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3403],\n",
            "        [0.3488],\n",
            "        [0.3572],\n",
            "        [0.3656],\n",
            "        [0.3741],\n",
            "        [0.3825],\n",
            "        [0.3910],\n",
            "        [0.3994],\n",
            "        [0.4079],\n",
            "        [0.4163]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.07638873159885406\n",
            "\n",
            "Epoch: 8 | Loss: 0.07638873159885406\n",
            "Model weights: tensor([0.4258])\n",
            "Model bias: tensor([0.3463])\n",
            "Prediction before backprop: tensor([[0.3403],\n",
            "        [0.3488],\n",
            "        [0.3572],\n",
            "        [0.3656],\n",
            "        [0.3741],\n",
            "        [0.3825],\n",
            "        [0.3910],\n",
            "        [0.3994],\n",
            "        [0.4079],\n",
            "        [0.4163]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3463],\n",
            "        [0.3548],\n",
            "        [0.3633],\n",
            "        [0.3719],\n",
            "        [0.3804],\n",
            "        [0.3889],\n",
            "        [0.3974],\n",
            "        [0.4059],\n",
            "        [0.4144],\n",
            "        [0.4230]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.07160007208585739\n",
            "\n",
            "Epoch: 9 | Loss: 0.07160007208585739\n",
            "Model weights: tensor([0.4293])\n",
            "Model bias: tensor([0.3518])\n",
            "Prediction before backprop: tensor([[0.3463],\n",
            "        [0.3548],\n",
            "        [0.3633],\n",
            "        [0.3719],\n",
            "        [0.3804],\n",
            "        [0.3889],\n",
            "        [0.3974],\n",
            "        [0.4059],\n",
            "        [0.4144],\n",
            "        [0.4230]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3518],\n",
            "        [0.3604],\n",
            "        [0.3690],\n",
            "        [0.3776],\n",
            "        [0.3862],\n",
            "        [0.3947],\n",
            "        [0.4033],\n",
            "        [0.4119],\n",
            "        [0.4205],\n",
            "        [0.4291]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.06747635453939438\n",
            "\n",
            "Epoch: 10 | Loss: 0.06747635453939438\n",
            "Model weights: tensor([0.4328])\n",
            "Model bias: tensor([0.3568])\n",
            "Prediction before backprop: tensor([[0.3518],\n",
            "        [0.3604],\n",
            "        [0.3690],\n",
            "        [0.3776],\n",
            "        [0.3862],\n",
            "        [0.3947],\n",
            "        [0.4033],\n",
            "        [0.4119],\n",
            "        [0.4205],\n",
            "        [0.4291]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3568],\n",
            "        [0.3655],\n",
            "        [0.3741],\n",
            "        [0.3828],\n",
            "        [0.3914],\n",
            "        [0.4001],\n",
            "        [0.4087],\n",
            "        [0.4174],\n",
            "        [0.4261],\n",
            "        [0.4347]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.06395438313484192\n",
            "\n",
            "Epoch: 11 | Loss: 0.06395438313484192\n",
            "Model weights: tensor([0.4361])\n",
            "Model bias: tensor([0.3613])\n",
            "Prediction before backprop: tensor([[0.3568],\n",
            "        [0.3655],\n",
            "        [0.3741],\n",
            "        [0.3828],\n",
            "        [0.3914],\n",
            "        [0.4001],\n",
            "        [0.4087],\n",
            "        [0.4174],\n",
            "        [0.4261],\n",
            "        [0.4347]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3613],\n",
            "        [0.3700],\n",
            "        [0.3788],\n",
            "        [0.3875],\n",
            "        [0.3962],\n",
            "        [0.4049],\n",
            "        [0.4136],\n",
            "        [0.4224],\n",
            "        [0.4311],\n",
            "        [0.4398]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.06097004935145378\n",
            "\n",
            "Epoch: 12 | Loss: 0.06097004935145378\n",
            "Model weights: tensor([0.4394])\n",
            "Model bias: tensor([0.3653])\n",
            "Prediction before backprop: tensor([[0.3613],\n",
            "        [0.3700],\n",
            "        [0.3788],\n",
            "        [0.3875],\n",
            "        [0.3962],\n",
            "        [0.4049],\n",
            "        [0.4136],\n",
            "        [0.4224],\n",
            "        [0.4311],\n",
            "        [0.4398]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3653],\n",
            "        [0.3741],\n",
            "        [0.3829],\n",
            "        [0.3917],\n",
            "        [0.4005],\n",
            "        [0.4092],\n",
            "        [0.4180],\n",
            "        [0.4268],\n",
            "        [0.4356],\n",
            "        [0.4444]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.05845819041132927\n",
            "\n",
            "Epoch: 13 | Loss: 0.05845819041132927\n",
            "Model weights: tensor([0.4425])\n",
            "Model bias: tensor([0.3688])\n",
            "Prediction before backprop: tensor([[0.3653],\n",
            "        [0.3741],\n",
            "        [0.3829],\n",
            "        [0.3917],\n",
            "        [0.4005],\n",
            "        [0.4092],\n",
            "        [0.4180],\n",
            "        [0.4268],\n",
            "        [0.4356],\n",
            "        [0.4444]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3688],\n",
            "        [0.3777],\n",
            "        [0.3865],\n",
            "        [0.3954],\n",
            "        [0.4042],\n",
            "        [0.4131],\n",
            "        [0.4219],\n",
            "        [0.4308],\n",
            "        [0.4396],\n",
            "        [0.4485]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.05635259300470352\n",
            "\n",
            "Epoch: 14 | Loss: 0.05635259300470352\n",
            "Model weights: tensor([0.4455])\n",
            "Model bias: tensor([0.3718])\n",
            "Prediction before backprop: tensor([[0.3688],\n",
            "        [0.3777],\n",
            "        [0.3865],\n",
            "        [0.3954],\n",
            "        [0.4042],\n",
            "        [0.4131],\n",
            "        [0.4219],\n",
            "        [0.4308],\n",
            "        [0.4396],\n",
            "        [0.4485]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3718],\n",
            "        [0.3807],\n",
            "        [0.3896],\n",
            "        [0.3985],\n",
            "        [0.4074],\n",
            "        [0.4164],\n",
            "        [0.4253],\n",
            "        [0.4342],\n",
            "        [0.4431],\n",
            "        [0.4520]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.0545857772231102\n",
            "\n",
            "Epoch: 15 | Loss: 0.0545857772231102\n",
            "Model weights: tensor([0.4483])\n",
            "Model bias: tensor([0.3743])\n",
            "Prediction before backprop: tensor([[0.3718],\n",
            "        [0.3807],\n",
            "        [0.3896],\n",
            "        [0.3985],\n",
            "        [0.4074],\n",
            "        [0.4164],\n",
            "        [0.4253],\n",
            "        [0.4342],\n",
            "        [0.4431],\n",
            "        [0.4520]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3743],\n",
            "        [0.3833],\n",
            "        [0.3922],\n",
            "        [0.4012],\n",
            "        [0.4102],\n",
            "        [0.4191],\n",
            "        [0.4281],\n",
            "        [0.4371],\n",
            "        [0.4460],\n",
            "        [0.4550]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.053148526698350906\n",
            "\n",
            "Epoch: 16 | Loss: 0.053148526698350906\n",
            "Model weights: tensor([0.4512])\n",
            "Model bias: tensor([0.3768])\n",
            "Prediction before backprop: tensor([[0.3743],\n",
            "        [0.3833],\n",
            "        [0.3922],\n",
            "        [0.4012],\n",
            "        [0.4102],\n",
            "        [0.4191],\n",
            "        [0.4281],\n",
            "        [0.4371],\n",
            "        [0.4460],\n",
            "        [0.4550]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3768],\n",
            "        [0.3858],\n",
            "        [0.3949],\n",
            "        [0.4039],\n",
            "        [0.4129],\n",
            "        [0.4219],\n",
            "        [0.4310],\n",
            "        [0.4400],\n",
            "        [0.4490],\n",
            "        [0.4580]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.05181945487856865\n",
            "\n",
            "Epoch: 17 | Loss: 0.05181945487856865\n",
            "Model weights: tensor([0.4539])\n",
            "Model bias: tensor([0.3788])\n",
            "Prediction before backprop: tensor([[0.3768],\n",
            "        [0.3858],\n",
            "        [0.3949],\n",
            "        [0.4039],\n",
            "        [0.4129],\n",
            "        [0.4219],\n",
            "        [0.4310],\n",
            "        [0.4400],\n",
            "        [0.4490],\n",
            "        [0.4580]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3788],\n",
            "        [0.3879],\n",
            "        [0.3970],\n",
            "        [0.4060],\n",
            "        [0.4151],\n",
            "        [0.4242],\n",
            "        [0.4333],\n",
            "        [0.4424],\n",
            "        [0.4514],\n",
            "        [0.4605]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.05069301277399063\n",
            "\n",
            "Epoch: 18 | Loss: 0.05069301277399063\n",
            "Model weights: tensor([0.4564])\n",
            "Model bias: tensor([0.3803])\n",
            "Prediction before backprop: tensor([[0.3788],\n",
            "        [0.3879],\n",
            "        [0.3970],\n",
            "        [0.4060],\n",
            "        [0.4151],\n",
            "        [0.4242],\n",
            "        [0.4333],\n",
            "        [0.4424],\n",
            "        [0.4514],\n",
            "        [0.4605]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3803],\n",
            "        [0.3894],\n",
            "        [0.3986],\n",
            "        [0.4077],\n",
            "        [0.4168],\n",
            "        [0.4260],\n",
            "        [0.4351],\n",
            "        [0.4442],\n",
            "        [0.4533],\n",
            "        [0.4625]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.0498228520154953\n",
            "\n",
            "Epoch: 19 | Loss: 0.0498228520154953\n",
            "Model weights: tensor([0.4590])\n",
            "Model bias: tensor([0.3818])\n",
            "Prediction before backprop: tensor([[0.3803],\n",
            "        [0.3894],\n",
            "        [0.3986],\n",
            "        [0.4077],\n",
            "        [0.4168],\n",
            "        [0.4260],\n",
            "        [0.4351],\n",
            "        [0.4442],\n",
            "        [0.4533],\n",
            "        [0.4625]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3818],\n",
            "        [0.3910],\n",
            "        [0.4002],\n",
            "        [0.4093],\n",
            "        [0.4185],\n",
            "        [0.4277],\n",
            "        [0.4369],\n",
            "        [0.4461],\n",
            "        [0.4552],\n",
            "        [0.4644]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04895269125699997\n",
            "\n",
            "Epoch: 20 | Loss: 0.04895269125699997\n",
            "Model weights: tensor([0.4615])\n",
            "Model bias: tensor([0.3833])\n",
            "Prediction before backprop: tensor([[0.3818],\n",
            "        [0.3910],\n",
            "        [0.4002],\n",
            "        [0.4093],\n",
            "        [0.4185],\n",
            "        [0.4277],\n",
            "        [0.4369],\n",
            "        [0.4461],\n",
            "        [0.4552],\n",
            "        [0.4644]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3833],\n",
            "        [0.3925],\n",
            "        [0.4018],\n",
            "        [0.4110],\n",
            "        [0.4202],\n",
            "        [0.4295],\n",
            "        [0.4387],\n",
            "        [0.4479],\n",
            "        [0.4571],\n",
            "        [0.4664]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04819351062178612\n",
            "\n",
            "Epoch: 21 | Loss: 0.04819351062178612\n",
            "Model weights: tensor([0.4639])\n",
            "Model bias: tensor([0.3843])\n",
            "Prediction before backprop: tensor([[0.3833],\n",
            "        [0.3925],\n",
            "        [0.4018],\n",
            "        [0.4110],\n",
            "        [0.4202],\n",
            "        [0.4295],\n",
            "        [0.4387],\n",
            "        [0.4479],\n",
            "        [0.4571],\n",
            "        [0.4664]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3843],\n",
            "        [0.3936],\n",
            "        [0.4029],\n",
            "        [0.4121],\n",
            "        [0.4214],\n",
            "        [0.4307],\n",
            "        [0.4400],\n",
            "        [0.4493],\n",
            "        [0.4585],\n",
            "        [0.4678]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.047531817108392715\n",
            "\n",
            "Epoch: 22 | Loss: 0.047531817108392715\n",
            "Model weights: tensor([0.4662])\n",
            "Model bias: tensor([0.3853])\n",
            "Prediction before backprop: tensor([[0.3843],\n",
            "        [0.3936],\n",
            "        [0.4029],\n",
            "        [0.4121],\n",
            "        [0.4214],\n",
            "        [0.4307],\n",
            "        [0.4400],\n",
            "        [0.4493],\n",
            "        [0.4585],\n",
            "        [0.4678]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3853],\n",
            "        [0.3946],\n",
            "        [0.4040],\n",
            "        [0.4133],\n",
            "        [0.4226],\n",
            "        [0.4319],\n",
            "        [0.4413],\n",
            "        [0.4506],\n",
            "        [0.4599],\n",
            "        [0.4692]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04692792519927025\n",
            "\n",
            "Epoch: 23 | Loss: 0.04692792519927025\n",
            "Model weights: tensor([0.4684])\n",
            "Model bias: tensor([0.3858])\n",
            "Prediction before backprop: tensor([[0.3853],\n",
            "        [0.3946],\n",
            "        [0.4040],\n",
            "        [0.4133],\n",
            "        [0.4226],\n",
            "        [0.4319],\n",
            "        [0.4413],\n",
            "        [0.4506],\n",
            "        [0.4599],\n",
            "        [0.4692]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3858],\n",
            "        [0.3952],\n",
            "        [0.4045],\n",
            "        [0.4139],\n",
            "        [0.4233],\n",
            "        [0.4327],\n",
            "        [0.4420],\n",
            "        [0.4514],\n",
            "        [0.4608],\n",
            "        [0.4701]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04642331600189209\n",
            "\n",
            "Epoch: 24 | Loss: 0.04642331600189209\n",
            "Model weights: tensor([0.4706])\n",
            "Model bias: tensor([0.3863])\n",
            "Prediction before backprop: tensor([[0.3858],\n",
            "        [0.3952],\n",
            "        [0.4045],\n",
            "        [0.4139],\n",
            "        [0.4233],\n",
            "        [0.4327],\n",
            "        [0.4420],\n",
            "        [0.4514],\n",
            "        [0.4608],\n",
            "        [0.4701]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3863],\n",
            "        [0.3957],\n",
            "        [0.4051],\n",
            "        [0.4145],\n",
            "        [0.4240],\n",
            "        [0.4334],\n",
            "        [0.4428],\n",
            "        [0.4522],\n",
            "        [0.4616],\n",
            "        [0.4710]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04591871052980423\n",
            "\n",
            "Epoch: 25 | Loss: 0.04591871052980423\n",
            "Model weights: tensor([0.4728])\n",
            "Model bias: tensor([0.3868])\n",
            "Prediction before backprop: tensor([[0.3863],\n",
            "        [0.3957],\n",
            "        [0.4051],\n",
            "        [0.4145],\n",
            "        [0.4240],\n",
            "        [0.4334],\n",
            "        [0.4428],\n",
            "        [0.4522],\n",
            "        [0.4616],\n",
            "        [0.4710]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3868],\n",
            "        [0.3963],\n",
            "        [0.4057],\n",
            "        [0.4152],\n",
            "        [0.4246],\n",
            "        [0.4341],\n",
            "        [0.4435],\n",
            "        [0.4530],\n",
            "        [0.4625],\n",
            "        [0.4719]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04543796554207802\n",
            "\n",
            "Epoch: 26 | Loss: 0.04543796554207802\n",
            "Model weights: tensor([0.4748])\n",
            "Model bias: tensor([0.3868])\n",
            "Prediction before backprop: tensor([[0.3868],\n",
            "        [0.3963],\n",
            "        [0.4057],\n",
            "        [0.4152],\n",
            "        [0.4246],\n",
            "        [0.4341],\n",
            "        [0.4435],\n",
            "        [0.4530],\n",
            "        [0.4625],\n",
            "        [0.4719]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3868],\n",
            "        [0.3963],\n",
            "        [0.4058],\n",
            "        [0.4153],\n",
            "        [0.4248],\n",
            "        [0.4343],\n",
            "        [0.4438],\n",
            "        [0.4533],\n",
            "        [0.4628],\n",
            "        [0.4723]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04503796249628067\n",
            "\n",
            "Epoch: 27 | Loss: 0.04503796249628067\n",
            "Model weights: tensor([0.4768])\n",
            "Model bias: tensor([0.3868])\n",
            "Prediction before backprop: tensor([[0.3868],\n",
            "        [0.3963],\n",
            "        [0.4058],\n",
            "        [0.4153],\n",
            "        [0.4248],\n",
            "        [0.4343],\n",
            "        [0.4438],\n",
            "        [0.4533],\n",
            "        [0.4628],\n",
            "        [0.4723]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3868],\n",
            "        [0.3963],\n",
            "        [0.4059],\n",
            "        [0.4154],\n",
            "        [0.4250],\n",
            "        [0.4345],\n",
            "        [0.4440],\n",
            "        [0.4536],\n",
            "        [0.4631],\n",
            "        [0.4726]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04463795945048332\n",
            "\n",
            "Epoch: 28 | Loss: 0.04463795945048332\n",
            "Model weights: tensor([0.4788])\n",
            "Model bias: tensor([0.3868])\n",
            "Prediction before backprop: tensor([[0.3868],\n",
            "        [0.3963],\n",
            "        [0.4059],\n",
            "        [0.4154],\n",
            "        [0.4250],\n",
            "        [0.4345],\n",
            "        [0.4440],\n",
            "        [0.4536],\n",
            "        [0.4631],\n",
            "        [0.4726]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3868],\n",
            "        [0.3964],\n",
            "        [0.4060],\n",
            "        [0.4155],\n",
            "        [0.4251],\n",
            "        [0.4347],\n",
            "        [0.4443],\n",
            "        [0.4538],\n",
            "        [0.4634],\n",
            "        [0.4730]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04423796385526657\n",
            "\n",
            "Epoch: 29 | Loss: 0.04423796385526657\n",
            "Model weights: tensor([0.4808])\n",
            "Model bias: tensor([0.3868])\n",
            "Prediction before backprop: tensor([[0.3868],\n",
            "        [0.3964],\n",
            "        [0.4060],\n",
            "        [0.4155],\n",
            "        [0.4251],\n",
            "        [0.4347],\n",
            "        [0.4443],\n",
            "        [0.4538],\n",
            "        [0.4634],\n",
            "        [0.4730]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3868],\n",
            "        [0.3964],\n",
            "        [0.4060],\n",
            "        [0.4157],\n",
            "        [0.4253],\n",
            "        [0.4349],\n",
            "        [0.4445],\n",
            "        [0.4541],\n",
            "        [0.4637],\n",
            "        [0.4734]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04383796453475952\n",
            "\n",
            "Epoch: 30 | Loss: 0.04383796453475952\n",
            "Model weights: tensor([0.4828])\n",
            "Model bias: tensor([0.3868])\n",
            "Prediction before backprop: tensor([[0.3868],\n",
            "        [0.3964],\n",
            "        [0.4060],\n",
            "        [0.4157],\n",
            "        [0.4253],\n",
            "        [0.4349],\n",
            "        [0.4445],\n",
            "        [0.4541],\n",
            "        [0.4637],\n",
            "        [0.4734]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3868],\n",
            "        [0.3965],\n",
            "        [0.4061],\n",
            "        [0.4158],\n",
            "        [0.4254],\n",
            "        [0.4351],\n",
            "        [0.4447],\n",
            "        [0.4544],\n",
            "        [0.4641],\n",
            "        [0.4737]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04343796148896217\n",
            "\n",
            "Epoch: 31 | Loss: 0.04343796148896217\n",
            "Model weights: tensor([0.4848])\n",
            "Model bias: tensor([0.3868])\n",
            "Prediction before backprop: tensor([[0.3868],\n",
            "        [0.3965],\n",
            "        [0.4061],\n",
            "        [0.4158],\n",
            "        [0.4254],\n",
            "        [0.4351],\n",
            "        [0.4447],\n",
            "        [0.4544],\n",
            "        [0.4641],\n",
            "        [0.4737]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3868],\n",
            "        [0.3965],\n",
            "        [0.4062],\n",
            "        [0.4159],\n",
            "        [0.4256],\n",
            "        [0.4353],\n",
            "        [0.4450],\n",
            "        [0.4547],\n",
            "        [0.4644],\n",
            "        [0.4741]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.043074630200862885\n",
            "\n",
            "Epoch: 32 | Loss: 0.043074630200862885\n",
            "Model weights: tensor([0.4866])\n",
            "Model bias: tensor([0.3863])\n",
            "Prediction before backprop: tensor([[0.3868],\n",
            "        [0.3965],\n",
            "        [0.4062],\n",
            "        [0.4159],\n",
            "        [0.4256],\n",
            "        [0.4353],\n",
            "        [0.4450],\n",
            "        [0.4547],\n",
            "        [0.4644],\n",
            "        [0.4741]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3863],\n",
            "        [0.3960],\n",
            "        [0.4058],\n",
            "        [0.4155],\n",
            "        [0.4252],\n",
            "        [0.4350],\n",
            "        [0.4447],\n",
            "        [0.4544],\n",
            "        [0.4642],\n",
            "        [0.4739]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04272563382983208\n",
            "\n",
            "Epoch: 33 | Loss: 0.04272563382983208\n",
            "Model weights: tensor([0.4884])\n",
            "Model bias: tensor([0.3858])\n",
            "Prediction before backprop: tensor([[0.3863],\n",
            "        [0.3960],\n",
            "        [0.4058],\n",
            "        [0.4155],\n",
            "        [0.4252],\n",
            "        [0.4350],\n",
            "        [0.4447],\n",
            "        [0.4544],\n",
            "        [0.4642],\n",
            "        [0.4739]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3858],\n",
            "        [0.3956],\n",
            "        [0.4053],\n",
            "        [0.4151],\n",
            "        [0.4249],\n",
            "        [0.4347],\n",
            "        [0.4444],\n",
            "        [0.4542],\n",
            "        [0.4640],\n",
            "        [0.4737]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04237663000822067\n",
            "\n",
            "Epoch: 34 | Loss: 0.04237663000822067\n",
            "Model weights: tensor([0.4902])\n",
            "Model bias: tensor([0.3853])\n",
            "Prediction before backprop: tensor([[0.3858],\n",
            "        [0.3956],\n",
            "        [0.4053],\n",
            "        [0.4151],\n",
            "        [0.4249],\n",
            "        [0.4347],\n",
            "        [0.4444],\n",
            "        [0.4542],\n",
            "        [0.4640],\n",
            "        [0.4737]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3853],\n",
            "        [0.3951],\n",
            "        [0.4049],\n",
            "        [0.4147],\n",
            "        [0.4245],\n",
            "        [0.4343],\n",
            "        [0.4441],\n",
            "        [0.4539],\n",
            "        [0.4637],\n",
            "        [0.4735]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04202762991189957\n",
            "\n",
            "Epoch: 35 | Loss: 0.04202762991189957\n",
            "Model weights: tensor([0.4920])\n",
            "Model bias: tensor([0.3848])\n",
            "Prediction before backprop: tensor([[0.3853],\n",
            "        [0.3951],\n",
            "        [0.4049],\n",
            "        [0.4147],\n",
            "        [0.4245],\n",
            "        [0.4343],\n",
            "        [0.4441],\n",
            "        [0.4539],\n",
            "        [0.4637],\n",
            "        [0.4735]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3848],\n",
            "        [0.3946],\n",
            "        [0.4045],\n",
            "        [0.4143],\n",
            "        [0.4242],\n",
            "        [0.4340],\n",
            "        [0.4439],\n",
            "        [0.4537],\n",
            "        [0.4635],\n",
            "        [0.4734]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04167863354086876\n",
            "\n",
            "Epoch: 36 | Loss: 0.04167863354086876\n",
            "Model weights: tensor([0.4938])\n",
            "Model bias: tensor([0.3843])\n",
            "Prediction before backprop: tensor([[0.3848],\n",
            "        [0.3946],\n",
            "        [0.4045],\n",
            "        [0.4143],\n",
            "        [0.4242],\n",
            "        [0.4340],\n",
            "        [0.4439],\n",
            "        [0.4537],\n",
            "        [0.4635],\n",
            "        [0.4734]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3843],\n",
            "        [0.3942],\n",
            "        [0.4041],\n",
            "        [0.4139],\n",
            "        [0.4238],\n",
            "        [0.4337],\n",
            "        [0.4436],\n",
            "        [0.4534],\n",
            "        [0.4633],\n",
            "        [0.4732]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04132963344454765\n",
            "\n",
            "Epoch: 37 | Loss: 0.04132963344454765\n",
            "Model weights: tensor([0.4956])\n",
            "Model bias: tensor([0.3838])\n",
            "Prediction before backprop: tensor([[0.3843],\n",
            "        [0.3942],\n",
            "        [0.4041],\n",
            "        [0.4139],\n",
            "        [0.4238],\n",
            "        [0.4337],\n",
            "        [0.4436],\n",
            "        [0.4534],\n",
            "        [0.4633],\n",
            "        [0.4732]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3838],\n",
            "        [0.3937],\n",
            "        [0.4036],\n",
            "        [0.4135],\n",
            "        [0.4235],\n",
            "        [0.4334],\n",
            "        [0.4433],\n",
            "        [0.4532],\n",
            "        [0.4631],\n",
            "        [0.4730]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04098063334822655\n",
            "\n",
            "Epoch: 38 | Loss: 0.04098063334822655\n",
            "Model weights: tensor([0.4974])\n",
            "Model bias: tensor([0.3833])\n",
            "Prediction before backprop: tensor([[0.3838],\n",
            "        [0.3937],\n",
            "        [0.4036],\n",
            "        [0.4135],\n",
            "        [0.4235],\n",
            "        [0.4334],\n",
            "        [0.4433],\n",
            "        [0.4532],\n",
            "        [0.4631],\n",
            "        [0.4730]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3833],\n",
            "        [0.3933],\n",
            "        [0.4032],\n",
            "        [0.4132],\n",
            "        [0.4231],\n",
            "        [0.4331],\n",
            "        [0.4430],\n",
            "        [0.4529],\n",
            "        [0.4629],\n",
            "        [0.4728]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.04063162952661514\n",
            "\n",
            "Epoch: 39 | Loss: 0.04063162952661514\n",
            "Model weights: tensor([0.4992])\n",
            "Model bias: tensor([0.3828])\n",
            "Prediction before backprop: tensor([[0.3833],\n",
            "        [0.3933],\n",
            "        [0.4032],\n",
            "        [0.4132],\n",
            "        [0.4231],\n",
            "        [0.4331],\n",
            "        [0.4430],\n",
            "        [0.4529],\n",
            "        [0.4629],\n",
            "        [0.4728]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3828],\n",
            "        [0.3928],\n",
            "        [0.4028],\n",
            "        [0.4128],\n",
            "        [0.4227],\n",
            "        [0.4327],\n",
            "        [0.4427],\n",
            "        [0.4527],\n",
            "        [0.4627],\n",
            "        [0.4727]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.040282636880874634\n",
            "\n",
            "Epoch: 40 | Loss: 0.040282636880874634\n",
            "Model weights: tensor([0.5010])\n",
            "Model bias: tensor([0.3823])\n",
            "Prediction before backprop: tensor([[0.3828],\n",
            "        [0.3928],\n",
            "        [0.4028],\n",
            "        [0.4128],\n",
            "        [0.4227],\n",
            "        [0.4327],\n",
            "        [0.4427],\n",
            "        [0.4527],\n",
            "        [0.4627],\n",
            "        [0.4727]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3823],\n",
            "        [0.3923],\n",
            "        [0.4023],\n",
            "        [0.4124],\n",
            "        [0.4224],\n",
            "        [0.4324],\n",
            "        [0.4424],\n",
            "        [0.4525],\n",
            "        [0.4625],\n",
            "        [0.4725]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.039933640509843826\n",
            "\n",
            "Epoch: 41 | Loss: 0.039933640509843826\n",
            "Model weights: tensor([0.5028])\n",
            "Model bias: tensor([0.3818])\n",
            "Prediction before backprop: tensor([[0.3823],\n",
            "        [0.3923],\n",
            "        [0.4023],\n",
            "        [0.4124],\n",
            "        [0.4224],\n",
            "        [0.4324],\n",
            "        [0.4424],\n",
            "        [0.4525],\n",
            "        [0.4625],\n",
            "        [0.4725]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3818],\n",
            "        [0.3919],\n",
            "        [0.4019],\n",
            "        [0.4120],\n",
            "        [0.4220],\n",
            "        [0.4321],\n",
            "        [0.4421],\n",
            "        [0.4522],\n",
            "        [0.4623],\n",
            "        [0.4723]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03958464413881302\n",
            "\n",
            "Epoch: 42 | Loss: 0.03958464413881302\n",
            "Model weights: tensor([0.5046])\n",
            "Model bias: tensor([0.3813])\n",
            "Prediction before backprop: tensor([[0.3818],\n",
            "        [0.3919],\n",
            "        [0.4019],\n",
            "        [0.4120],\n",
            "        [0.4220],\n",
            "        [0.4321],\n",
            "        [0.4421],\n",
            "        [0.4522],\n",
            "        [0.4623],\n",
            "        [0.4723]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3813],\n",
            "        [0.3914],\n",
            "        [0.4015],\n",
            "        [0.4116],\n",
            "        [0.4217],\n",
            "        [0.4318],\n",
            "        [0.4419],\n",
            "        [0.4520],\n",
            "        [0.4620],\n",
            "        [0.4721]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03923564404249191\n",
            "\n",
            "Epoch: 43 | Loss: 0.03923564404249191\n",
            "Model weights: tensor([0.5064])\n",
            "Model bias: tensor([0.3808])\n",
            "Prediction before backprop: tensor([[0.3813],\n",
            "        [0.3914],\n",
            "        [0.4015],\n",
            "        [0.4116],\n",
            "        [0.4217],\n",
            "        [0.4318],\n",
            "        [0.4419],\n",
            "        [0.4520],\n",
            "        [0.4620],\n",
            "        [0.4721]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3808],\n",
            "        [0.3909],\n",
            "        [0.4011],\n",
            "        [0.4112],\n",
            "        [0.4213],\n",
            "        [0.4315],\n",
            "        [0.4416],\n",
            "        [0.4517],\n",
            "        [0.4618],\n",
            "        [0.4720]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03888664394617081\n",
            "\n",
            "Epoch: 44 | Loss: 0.03888664394617081\n",
            "Model weights: tensor([0.5082])\n",
            "Model bias: tensor([0.3803])\n",
            "Prediction before backprop: tensor([[0.3808],\n",
            "        [0.3909],\n",
            "        [0.4011],\n",
            "        [0.4112],\n",
            "        [0.4213],\n",
            "        [0.4315],\n",
            "        [0.4416],\n",
            "        [0.4517],\n",
            "        [0.4618],\n",
            "        [0.4720]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3803],\n",
            "        [0.3905],\n",
            "        [0.4006],\n",
            "        [0.4108],\n",
            "        [0.4210],\n",
            "        [0.4311],\n",
            "        [0.4413],\n",
            "        [0.4515],\n",
            "        [0.4616],\n",
            "        [0.4718]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.0385376438498497\n",
            "\n",
            "Epoch: 45 | Loss: 0.0385376438498497\n",
            "Model weights: tensor([0.5100])\n",
            "Model bias: tensor([0.3798])\n",
            "Prediction before backprop: tensor([[0.3803],\n",
            "        [0.3905],\n",
            "        [0.4006],\n",
            "        [0.4108],\n",
            "        [0.4210],\n",
            "        [0.4311],\n",
            "        [0.4413],\n",
            "        [0.4515],\n",
            "        [0.4616],\n",
            "        [0.4718]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3798],\n",
            "        [0.3900],\n",
            "        [0.4002],\n",
            "        [0.4104],\n",
            "        [0.4206],\n",
            "        [0.4308],\n",
            "        [0.4410],\n",
            "        [0.4512],\n",
            "        [0.4614],\n",
            "        [0.4716]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03818932920694351\n",
            "\n",
            "Epoch: 46 | Loss: 0.03818932920694351\n",
            "Model weights: tensor([0.5116])\n",
            "Model bias: tensor([0.3788])\n",
            "Prediction before backprop: tensor([[0.3798],\n",
            "        [0.3900],\n",
            "        [0.4002],\n",
            "        [0.4104],\n",
            "        [0.4206],\n",
            "        [0.4308],\n",
            "        [0.4410],\n",
            "        [0.4512],\n",
            "        [0.4614],\n",
            "        [0.4716]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3788],\n",
            "        [0.3890],\n",
            "        [0.3993],\n",
            "        [0.4095],\n",
            "        [0.4197],\n",
            "        [0.4300],\n",
            "        [0.4402],\n",
            "        [0.4504],\n",
            "        [0.4607],\n",
            "        [0.4709]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03785243630409241\n",
            "\n",
            "Epoch: 47 | Loss: 0.03785243630409241\n",
            "Model weights: tensor([0.5134])\n",
            "Model bias: tensor([0.3783])\n",
            "Prediction before backprop: tensor([[0.3788],\n",
            "        [0.3890],\n",
            "        [0.3993],\n",
            "        [0.4095],\n",
            "        [0.4197],\n",
            "        [0.4300],\n",
            "        [0.4402],\n",
            "        [0.4504],\n",
            "        [0.4607],\n",
            "        [0.4709]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3783],\n",
            "        [0.3886],\n",
            "        [0.3988],\n",
            "        [0.4091],\n",
            "        [0.4194],\n",
            "        [0.4296],\n",
            "        [0.4399],\n",
            "        [0.4502],\n",
            "        [0.4605],\n",
            "        [0.4707]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.0375034399330616\n",
            "\n",
            "Epoch: 48 | Loss: 0.0375034399330616\n",
            "Model weights: tensor([0.5152])\n",
            "Model bias: tensor([0.3778])\n",
            "Prediction before backprop: tensor([[0.3783],\n",
            "        [0.3886],\n",
            "        [0.3988],\n",
            "        [0.4091],\n",
            "        [0.4194],\n",
            "        [0.4296],\n",
            "        [0.4399],\n",
            "        [0.4502],\n",
            "        [0.4605],\n",
            "        [0.4707]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3778],\n",
            "        [0.3881],\n",
            "        [0.3984],\n",
            "        [0.4087],\n",
            "        [0.4190],\n",
            "        [0.4293],\n",
            "        [0.4396],\n",
            "        [0.4499],\n",
            "        [0.4602],\n",
            "        [0.4705]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.037164121866226196\n",
            "\n",
            "Epoch: 49 | Loss: 0.037164121866226196\n",
            "Model weights: tensor([0.5168])\n",
            "Model bias: tensor([0.3768])\n",
            "Prediction before backprop: tensor([[0.3778],\n",
            "        [0.3881],\n",
            "        [0.3984],\n",
            "        [0.4087],\n",
            "        [0.4190],\n",
            "        [0.4293],\n",
            "        [0.4396],\n",
            "        [0.4499],\n",
            "        [0.4602],\n",
            "        [0.4705]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3768],\n",
            "        [0.3871],\n",
            "        [0.3975],\n",
            "        [0.4078],\n",
            "        [0.4182],\n",
            "        [0.4285],\n",
            "        [0.4388],\n",
            "        [0.4492],\n",
            "        [0.4595],\n",
            "        [0.4698]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03681822493672371\n",
            "\n",
            "Epoch: 50 | Loss: 0.03681822493672371\n",
            "Model weights: tensor([0.5186])\n",
            "Model bias: tensor([0.3763])\n",
            "Prediction before backprop: tensor([[0.3768],\n",
            "        [0.3871],\n",
            "        [0.3975],\n",
            "        [0.4078],\n",
            "        [0.4182],\n",
            "        [0.4285],\n",
            "        [0.4388],\n",
            "        [0.4492],\n",
            "        [0.4595],\n",
            "        [0.4698]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3763],\n",
            "        [0.3867],\n",
            "        [0.3971],\n",
            "        [0.4074],\n",
            "        [0.4178],\n",
            "        [0.4282],\n",
            "        [0.4385],\n",
            "        [0.4489],\n",
            "        [0.4593],\n",
            "        [0.4697]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03647511452436447\n",
            "\n",
            "Epoch: 51 | Loss: 0.03647511452436447\n",
            "Model weights: tensor([0.5202])\n",
            "Model bias: tensor([0.3753])\n",
            "Prediction before backprop: tensor([[0.3763],\n",
            "        [0.3867],\n",
            "        [0.3971],\n",
            "        [0.4074],\n",
            "        [0.4178],\n",
            "        [0.4282],\n",
            "        [0.4385],\n",
            "        [0.4489],\n",
            "        [0.4593],\n",
            "        [0.4697]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3753],\n",
            "        [0.3857],\n",
            "        [0.3961],\n",
            "        [0.4065],\n",
            "        [0.4169],\n",
            "        [0.4273],\n",
            "        [0.4377],\n",
            "        [0.4481],\n",
            "        [0.4585],\n",
            "        [0.4689]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03613303601741791\n",
            "\n",
            "Epoch: 52 | Loss: 0.03613303601741791\n",
            "Model weights: tensor([0.5220])\n",
            "Model bias: tensor([0.3748])\n",
            "Prediction before backprop: tensor([[0.3753],\n",
            "        [0.3857],\n",
            "        [0.3961],\n",
            "        [0.4065],\n",
            "        [0.4169],\n",
            "        [0.4273],\n",
            "        [0.4377],\n",
            "        [0.4481],\n",
            "        [0.4585],\n",
            "        [0.4689]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3748],\n",
            "        [0.3852],\n",
            "        [0.3957],\n",
            "        [0.4061],\n",
            "        [0.4166],\n",
            "        [0.4270],\n",
            "        [0.4374],\n",
            "        [0.4479],\n",
            "        [0.4583],\n",
            "        [0.4688]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03578609973192215\n",
            "\n",
            "Epoch: 53 | Loss: 0.03578609973192215\n",
            "Model weights: tensor([0.5236])\n",
            "Model bias: tensor([0.3738])\n",
            "Prediction before backprop: tensor([[0.3748],\n",
            "        [0.3852],\n",
            "        [0.3957],\n",
            "        [0.4061],\n",
            "        [0.4166],\n",
            "        [0.4270],\n",
            "        [0.4374],\n",
            "        [0.4479],\n",
            "        [0.4583],\n",
            "        [0.4688]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3738],\n",
            "        [0.3843],\n",
            "        [0.3948],\n",
            "        [0.4052],\n",
            "        [0.4157],\n",
            "        [0.4262],\n",
            "        [0.4366],\n",
            "        [0.4471],\n",
            "        [0.4576],\n",
            "        [0.4681]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03544783592224121\n",
            "\n",
            "Epoch: 54 | Loss: 0.03544783592224121\n",
            "Model weights: tensor([0.5254])\n",
            "Model bias: tensor([0.3733])\n",
            "Prediction before backprop: tensor([[0.3738],\n",
            "        [0.3843],\n",
            "        [0.3948],\n",
            "        [0.4052],\n",
            "        [0.4157],\n",
            "        [0.4262],\n",
            "        [0.4366],\n",
            "        [0.4471],\n",
            "        [0.4576],\n",
            "        [0.4681]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3733],\n",
            "        [0.3838],\n",
            "        [0.3943],\n",
            "        [0.4048],\n",
            "        [0.4153],\n",
            "        [0.4258],\n",
            "        [0.4364],\n",
            "        [0.4469],\n",
            "        [0.4574],\n",
            "        [0.4679]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.035098835825920105\n",
            "\n",
            "Epoch: 55 | Loss: 0.035098835825920105\n",
            "Model weights: tensor([0.5272])\n",
            "Model bias: tensor([0.3728])\n",
            "Prediction before backprop: tensor([[0.3733],\n",
            "        [0.3838],\n",
            "        [0.3943],\n",
            "        [0.4048],\n",
            "        [0.4153],\n",
            "        [0.4258],\n",
            "        [0.4364],\n",
            "        [0.4469],\n",
            "        [0.4574],\n",
            "        [0.4679]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3728],\n",
            "        [0.3834],\n",
            "        [0.3939],\n",
            "        [0.4044],\n",
            "        [0.4150],\n",
            "        [0.4255],\n",
            "        [0.4361],\n",
            "        [0.4466],\n",
            "        [0.4572],\n",
            "        [0.4677]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03476089984178543\n",
            "\n",
            "Epoch: 56 | Loss: 0.03476089984178543\n",
            "Model weights: tensor([0.5288])\n",
            "Model bias: tensor([0.3718])\n",
            "Prediction before backprop: tensor([[0.3728],\n",
            "        [0.3834],\n",
            "        [0.3939],\n",
            "        [0.4044],\n",
            "        [0.4150],\n",
            "        [0.4255],\n",
            "        [0.4361],\n",
            "        [0.4466],\n",
            "        [0.4572],\n",
            "        [0.4677]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3718],\n",
            "        [0.3824],\n",
            "        [0.3930],\n",
            "        [0.4035],\n",
            "        [0.4141],\n",
            "        [0.4247],\n",
            "        [0.4353],\n",
            "        [0.4458],\n",
            "        [0.4564],\n",
            "        [0.4670]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03441363573074341\n",
            "\n",
            "Epoch: 57 | Loss: 0.03441363573074341\n",
            "Model weights: tensor([0.5306])\n",
            "Model bias: tensor([0.3713])\n",
            "Prediction before backprop: tensor([[0.3718],\n",
            "        [0.3824],\n",
            "        [0.3930],\n",
            "        [0.4035],\n",
            "        [0.4141],\n",
            "        [0.4247],\n",
            "        [0.4353],\n",
            "        [0.4458],\n",
            "        [0.4564],\n",
            "        [0.4670]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3713],\n",
            "        [0.3819],\n",
            "        [0.3925],\n",
            "        [0.4031],\n",
            "        [0.4138],\n",
            "        [0.4244],\n",
            "        [0.4350],\n",
            "        [0.4456],\n",
            "        [0.4562],\n",
            "        [0.4668]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03407188132405281\n",
            "\n",
            "Epoch: 58 | Loss: 0.03407188132405281\n",
            "Model weights: tensor([0.5322])\n",
            "Model bias: tensor([0.3703])\n",
            "Prediction before backprop: tensor([[0.3713],\n",
            "        [0.3819],\n",
            "        [0.3925],\n",
            "        [0.4031],\n",
            "        [0.4138],\n",
            "        [0.4244],\n",
            "        [0.4350],\n",
            "        [0.4456],\n",
            "        [0.4562],\n",
            "        [0.4668]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3703],\n",
            "        [0.3810],\n",
            "        [0.3916],\n",
            "        [0.4022],\n",
            "        [0.4129],\n",
            "        [0.4235],\n",
            "        [0.4342],\n",
            "        [0.4448],\n",
            "        [0.4555],\n",
            "        [0.4661]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03372843936085701\n",
            "\n",
            "Epoch: 59 | Loss: 0.03372843936085701\n",
            "Model weights: tensor([0.5340])\n",
            "Model bias: tensor([0.3698])\n",
            "Prediction before backprop: tensor([[0.3703],\n",
            "        [0.3810],\n",
            "        [0.3916],\n",
            "        [0.4022],\n",
            "        [0.4129],\n",
            "        [0.4235],\n",
            "        [0.4342],\n",
            "        [0.4448],\n",
            "        [0.4555],\n",
            "        [0.4661]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3698],\n",
            "        [0.3805],\n",
            "        [0.3912],\n",
            "        [0.4018],\n",
            "        [0.4125],\n",
            "        [0.4232],\n",
            "        [0.4339],\n",
            "        [0.4446],\n",
            "        [0.4552],\n",
            "        [0.4659]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03338287025690079\n",
            "\n",
            "Epoch: 60 | Loss: 0.03338287025690079\n",
            "Model weights: tensor([0.5355])\n",
            "Model bias: tensor([0.3688])\n",
            "Prediction before backprop: tensor([[0.3698],\n",
            "        [0.3805],\n",
            "        [0.3912],\n",
            "        [0.4018],\n",
            "        [0.4125],\n",
            "        [0.4232],\n",
            "        [0.4339],\n",
            "        [0.4446],\n",
            "        [0.4552],\n",
            "        [0.4659]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3688],\n",
            "        [0.3795],\n",
            "        [0.3902],\n",
            "        [0.4009],\n",
            "        [0.4117],\n",
            "        [0.4224],\n",
            "        [0.4331],\n",
            "        [0.4438],\n",
            "        [0.4545],\n",
            "        [0.4652]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.033043231815099716\n",
            "\n",
            "Epoch: 61 | Loss: 0.033043231815099716\n",
            "Model weights: tensor([0.5373])\n",
            "Model bias: tensor([0.3683])\n",
            "Prediction before backprop: tensor([[0.3688],\n",
            "        [0.3795],\n",
            "        [0.3902],\n",
            "        [0.4009],\n",
            "        [0.4117],\n",
            "        [0.4224],\n",
            "        [0.4331],\n",
            "        [0.4438],\n",
            "        [0.4545],\n",
            "        [0.4652]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3683],\n",
            "        [0.3791],\n",
            "        [0.3898],\n",
            "        [0.4006],\n",
            "        [0.4113],\n",
            "        [0.4220],\n",
            "        [0.4328],\n",
            "        [0.4435],\n",
            "        [0.4543],\n",
            "        [0.4650]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03269423171877861\n",
            "\n",
            "Epoch: 62 | Loss: 0.03269423171877861\n",
            "Model weights: tensor([0.5391])\n",
            "Model bias: tensor([0.3678])\n",
            "Prediction before backprop: tensor([[0.3683],\n",
            "        [0.3791],\n",
            "        [0.3898],\n",
            "        [0.4006],\n",
            "        [0.4113],\n",
            "        [0.4220],\n",
            "        [0.4328],\n",
            "        [0.4435],\n",
            "        [0.4543],\n",
            "        [0.4650]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3678],\n",
            "        [0.3786],\n",
            "        [0.3894],\n",
            "        [0.4002],\n",
            "        [0.4109],\n",
            "        [0.4217],\n",
            "        [0.4325],\n",
            "        [0.4433],\n",
            "        [0.4541],\n",
            "        [0.4649]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.032357655465602875\n",
            "\n",
            "Epoch: 63 | Loss: 0.032357655465602875\n",
            "Model weights: tensor([0.5407])\n",
            "Model bias: tensor([0.3668])\n",
            "Prediction before backprop: tensor([[0.3678],\n",
            "        [0.3786],\n",
            "        [0.3894],\n",
            "        [0.4002],\n",
            "        [0.4109],\n",
            "        [0.4217],\n",
            "        [0.4325],\n",
            "        [0.4433],\n",
            "        [0.4541],\n",
            "        [0.4649]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3668],\n",
            "        [0.3776],\n",
            "        [0.3884],\n",
            "        [0.3993],\n",
            "        [0.4101],\n",
            "        [0.4209],\n",
            "        [0.4317],\n",
            "        [0.4425],\n",
            "        [0.4533],\n",
            "        [0.4641]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03200903534889221\n",
            "\n",
            "Epoch: 64 | Loss: 0.03200903534889221\n",
            "Model weights: tensor([0.5425])\n",
            "Model bias: tensor([0.3663])\n",
            "Prediction before backprop: tensor([[0.3668],\n",
            "        [0.3776],\n",
            "        [0.3884],\n",
            "        [0.3993],\n",
            "        [0.4101],\n",
            "        [0.4209],\n",
            "        [0.4317],\n",
            "        [0.4425],\n",
            "        [0.4533],\n",
            "        [0.4641]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3663],\n",
            "        [0.3772],\n",
            "        [0.3880],\n",
            "        [0.3989],\n",
            "        [0.4097],\n",
            "        [0.4206],\n",
            "        [0.4314],\n",
            "        [0.4423],\n",
            "        [0.4531],\n",
            "        [0.4640]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03166864812374115\n",
            "\n",
            "Epoch: 65 | Loss: 0.03166864812374115\n",
            "Model weights: tensor([0.5441])\n",
            "Model bias: tensor([0.3653])\n",
            "Prediction before backprop: tensor([[0.3663],\n",
            "        [0.3772],\n",
            "        [0.3880],\n",
            "        [0.3989],\n",
            "        [0.4097],\n",
            "        [0.4206],\n",
            "        [0.4314],\n",
            "        [0.4423],\n",
            "        [0.4531],\n",
            "        [0.4640]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3653],\n",
            "        [0.3762],\n",
            "        [0.3871],\n",
            "        [0.3980],\n",
            "        [0.4088],\n",
            "        [0.4197],\n",
            "        [0.4306],\n",
            "        [0.4415],\n",
            "        [0.4524],\n",
            "        [0.4633]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.03132382780313492\n",
            "\n",
            "Epoch: 66 | Loss: 0.03132382780313492\n",
            "Model weights: tensor([0.5459])\n",
            "Model bias: tensor([0.3648])\n",
            "Prediction before backprop: tensor([[0.3653],\n",
            "        [0.3762],\n",
            "        [0.3871],\n",
            "        [0.3980],\n",
            "        [0.4088],\n",
            "        [0.4197],\n",
            "        [0.4306],\n",
            "        [0.4415],\n",
            "        [0.4524],\n",
            "        [0.4633]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3648],\n",
            "        [0.3757],\n",
            "        [0.3866],\n",
            "        [0.3976],\n",
            "        [0.4085],\n",
            "        [0.4194],\n",
            "        [0.4303],\n",
            "        [0.4412],\n",
            "        [0.4522],\n",
            "        [0.4631]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.030979642644524574\n",
            "\n",
            "Epoch: 67 | Loss: 0.030979642644524574\n",
            "Model weights: tensor([0.5475])\n",
            "Model bias: tensor([0.3638])\n",
            "Prediction before backprop: tensor([[0.3648],\n",
            "        [0.3757],\n",
            "        [0.3866],\n",
            "        [0.3976],\n",
            "        [0.4085],\n",
            "        [0.4194],\n",
            "        [0.4303],\n",
            "        [0.4412],\n",
            "        [0.4522],\n",
            "        [0.4631]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3638],\n",
            "        [0.3748],\n",
            "        [0.3857],\n",
            "        [0.3967],\n",
            "        [0.4076],\n",
            "        [0.4186],\n",
            "        [0.4295],\n",
            "        [0.4405],\n",
            "        [0.4514],\n",
            "        [0.4624]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.030638623982667923\n",
            "\n",
            "Epoch: 68 | Loss: 0.030638623982667923\n",
            "Model weights: tensor([0.5493])\n",
            "Model bias: tensor([0.3633])\n",
            "Prediction before backprop: tensor([[0.3638],\n",
            "        [0.3748],\n",
            "        [0.3857],\n",
            "        [0.3967],\n",
            "        [0.4076],\n",
            "        [0.4186],\n",
            "        [0.4295],\n",
            "        [0.4405],\n",
            "        [0.4514],\n",
            "        [0.4624]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3633],\n",
            "        [0.3743],\n",
            "        [0.3853],\n",
            "        [0.3963],\n",
            "        [0.4073],\n",
            "        [0.4182],\n",
            "        [0.4292],\n",
            "        [0.4402],\n",
            "        [0.4512],\n",
            "        [0.4622]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.0302906334400177\n",
            "\n",
            "Epoch: 69 | Loss: 0.0302906334400177\n",
            "Model weights: tensor([0.5509])\n",
            "Model bias: tensor([0.3623])\n",
            "Prediction before backprop: tensor([[0.3633],\n",
            "        [0.3743],\n",
            "        [0.3853],\n",
            "        [0.3963],\n",
            "        [0.4073],\n",
            "        [0.4182],\n",
            "        [0.4292],\n",
            "        [0.4402],\n",
            "        [0.4512],\n",
            "        [0.4622]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3623],\n",
            "        [0.3733],\n",
            "        [0.3843],\n",
            "        [0.3954],\n",
            "        [0.4064],\n",
            "        [0.4174],\n",
            "        [0.4284],\n",
            "        [0.4394],\n",
            "        [0.4505],\n",
            "        [0.4615]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.029953425750136375\n",
            "\n",
            "Epoch: 70 | Loss: 0.029953425750136375\n",
            "Model weights: tensor([0.5527])\n",
            "Model bias: tensor([0.3618])\n",
            "Prediction before backprop: tensor([[0.3623],\n",
            "        [0.3733],\n",
            "        [0.3843],\n",
            "        [0.3954],\n",
            "        [0.4064],\n",
            "        [0.4174],\n",
            "        [0.4284],\n",
            "        [0.4394],\n",
            "        [0.4505],\n",
            "        [0.4615]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3618],\n",
            "        [0.3729],\n",
            "        [0.3839],\n",
            "        [0.3950],\n",
            "        [0.4060],\n",
            "        [0.4171],\n",
            "        [0.4281],\n",
            "        [0.4392],\n",
            "        [0.4502],\n",
            "        [0.4613]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.02960442565381527\n",
            "\n",
            "Epoch: 71 | Loss: 0.02960442565381527\n",
            "Model weights: tensor([0.5545])\n",
            "Model bias: tensor([0.3613])\n",
            "Prediction before backprop: tensor([[0.3618],\n",
            "        [0.3729],\n",
            "        [0.3839],\n",
            "        [0.3950],\n",
            "        [0.4060],\n",
            "        [0.4171],\n",
            "        [0.4281],\n",
            "        [0.4392],\n",
            "        [0.4502],\n",
            "        [0.4613]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3613],\n",
            "        [0.3724],\n",
            "        [0.3835],\n",
            "        [0.3946],\n",
            "        [0.4057],\n",
            "        [0.4168],\n",
            "        [0.4278],\n",
            "        [0.4389],\n",
            "        [0.4500],\n",
            "        [0.4611]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.029265418648719788\n",
            "\n",
            "Epoch: 72 | Loss: 0.029265418648719788\n",
            "Model weights: tensor([0.5561])\n",
            "Model bias: tensor([0.3603])\n",
            "Prediction before backprop: tensor([[0.3613],\n",
            "        [0.3724],\n",
            "        [0.3835],\n",
            "        [0.3946],\n",
            "        [0.4057],\n",
            "        [0.4168],\n",
            "        [0.4278],\n",
            "        [0.4389],\n",
            "        [0.4500],\n",
            "        [0.4611]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3603],\n",
            "        [0.3714],\n",
            "        [0.3826],\n",
            "        [0.3937],\n",
            "        [0.4048],\n",
            "        [0.4159],\n",
            "        [0.4270],\n",
            "        [0.4382],\n",
            "        [0.4493],\n",
            "        [0.4604]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.028919223695993423\n",
            "\n",
            "Epoch: 73 | Loss: 0.028919223695993423\n",
            "Model weights: tensor([0.5579])\n",
            "Model bias: tensor([0.3598])\n",
            "Prediction before backprop: tensor([[0.3603],\n",
            "        [0.3714],\n",
            "        [0.3826],\n",
            "        [0.3937],\n",
            "        [0.4048],\n",
            "        [0.4159],\n",
            "        [0.4270],\n",
            "        [0.4382],\n",
            "        [0.4493],\n",
            "        [0.4604]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3598],\n",
            "        [0.3710],\n",
            "        [0.3821],\n",
            "        [0.3933],\n",
            "        [0.4044],\n",
            "        [0.4156],\n",
            "        [0.4268],\n",
            "        [0.4379],\n",
            "        [0.4491],\n",
            "        [0.4602]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.028576409444212914\n",
            "\n",
            "Epoch: 74 | Loss: 0.028576409444212914\n",
            "Model weights: tensor([0.5595])\n",
            "Model bias: tensor([0.3588])\n",
            "Prediction before backprop: tensor([[0.3598],\n",
            "        [0.3710],\n",
            "        [0.3821],\n",
            "        [0.3933],\n",
            "        [0.4044],\n",
            "        [0.4156],\n",
            "        [0.4268],\n",
            "        [0.4379],\n",
            "        [0.4491],\n",
            "        [0.4602]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3588],\n",
            "        [0.3700],\n",
            "        [0.3812],\n",
            "        [0.3924],\n",
            "        [0.4036],\n",
            "        [0.4148],\n",
            "        [0.4259],\n",
            "        [0.4371],\n",
            "        [0.4483],\n",
            "        [0.4595]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.028234025463461876\n",
            "\n",
            "Epoch: 75 | Loss: 0.028234025463461876\n",
            "Model weights: tensor([0.5613])\n",
            "Model bias: tensor([0.3583])\n",
            "Prediction before backprop: tensor([[0.3588],\n",
            "        [0.3700],\n",
            "        [0.3812],\n",
            "        [0.3924],\n",
            "        [0.4036],\n",
            "        [0.4148],\n",
            "        [0.4259],\n",
            "        [0.4371],\n",
            "        [0.4483],\n",
            "        [0.4595]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3583],\n",
            "        [0.3695],\n",
            "        [0.3808],\n",
            "        [0.3920],\n",
            "        [0.4032],\n",
            "        [0.4144],\n",
            "        [0.4257],\n",
            "        [0.4369],\n",
            "        [0.4481],\n",
            "        [0.4593]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.02788739837706089\n",
            "\n",
            "Epoch: 76 | Loss: 0.02788739837706089\n",
            "Model weights: tensor([0.5629])\n",
            "Model bias: tensor([0.3573])\n",
            "Prediction before backprop: tensor([[0.3583],\n",
            "        [0.3695],\n",
            "        [0.3808],\n",
            "        [0.3920],\n",
            "        [0.4032],\n",
            "        [0.4144],\n",
            "        [0.4257],\n",
            "        [0.4369],\n",
            "        [0.4481],\n",
            "        [0.4593]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3573],\n",
            "        [0.3686],\n",
            "        [0.3798],\n",
            "        [0.3911],\n",
            "        [0.4023],\n",
            "        [0.4136],\n",
            "        [0.4249],\n",
            "        [0.4361],\n",
            "        [0.4474],\n",
            "        [0.4586]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.02754882536828518\n",
            "\n",
            "Epoch: 77 | Loss: 0.02754882536828518\n",
            "Model weights: tensor([0.5647])\n",
            "Model bias: tensor([0.3568])\n",
            "Prediction before backprop: tensor([[0.3573],\n",
            "        [0.3686],\n",
            "        [0.3798],\n",
            "        [0.3911],\n",
            "        [0.4023],\n",
            "        [0.4136],\n",
            "        [0.4249],\n",
            "        [0.4361],\n",
            "        [0.4474],\n",
            "        [0.4586]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3568],\n",
            "        [0.3681],\n",
            "        [0.3794],\n",
            "        [0.3907],\n",
            "        [0.4020],\n",
            "        [0.4133],\n",
            "        [0.4246],\n",
            "        [0.4359],\n",
            "        [0.4472],\n",
            "        [0.4585]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.027199819684028625\n",
            "\n",
            "Epoch: 78 | Loss: 0.027199819684028625\n",
            "Model weights: tensor([0.5665])\n",
            "Model bias: tensor([0.3563])\n",
            "Prediction before backprop: tensor([[0.3568],\n",
            "        [0.3681],\n",
            "        [0.3794],\n",
            "        [0.3907],\n",
            "        [0.4020],\n",
            "        [0.4133],\n",
            "        [0.4246],\n",
            "        [0.4359],\n",
            "        [0.4472],\n",
            "        [0.4585]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3563],\n",
            "        [0.3676],\n",
            "        [0.3790],\n",
            "        [0.3903],\n",
            "        [0.4016],\n",
            "        [0.4130],\n",
            "        [0.4243],\n",
            "        [0.4356],\n",
            "        [0.4469],\n",
            "        [0.4583]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.026862185448408127\n",
            "\n",
            "Epoch: 79 | Loss: 0.026862185448408127\n",
            "Model weights: tensor([0.5681])\n",
            "Model bias: tensor([0.3553])\n",
            "Prediction before backprop: tensor([[0.3563],\n",
            "        [0.3676],\n",
            "        [0.3790],\n",
            "        [0.3903],\n",
            "        [0.4016],\n",
            "        [0.4130],\n",
            "        [0.4243],\n",
            "        [0.4356],\n",
            "        [0.4469],\n",
            "        [0.4583]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3553],\n",
            "        [0.3667],\n",
            "        [0.3780],\n",
            "        [0.3894],\n",
            "        [0.4008],\n",
            "        [0.4121],\n",
            "        [0.4235],\n",
            "        [0.4348],\n",
            "        [0.4462],\n",
            "        [0.4576]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.02651461586356163\n",
            "\n",
            "Epoch: 80 | Loss: 0.02651461586356163\n",
            "Model weights: tensor([0.5699])\n",
            "Model bias: tensor([0.3548])\n",
            "Prediction before backprop: tensor([[0.3553],\n",
            "        [0.3667],\n",
            "        [0.3780],\n",
            "        [0.3894],\n",
            "        [0.4008],\n",
            "        [0.4121],\n",
            "        [0.4235],\n",
            "        [0.4348],\n",
            "        [0.4462],\n",
            "        [0.4576]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3548],\n",
            "        [0.3662],\n",
            "        [0.3776],\n",
            "        [0.3890],\n",
            "        [0.4004],\n",
            "        [0.4118],\n",
            "        [0.4232],\n",
            "        [0.4346],\n",
            "        [0.4460],\n",
            "        [0.4574]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.026173178106546402\n",
            "\n",
            "Epoch: 81 | Loss: 0.026173178106546402\n",
            "Model weights: tensor([0.5715])\n",
            "Model bias: tensor([0.3538])\n",
            "Prediction before backprop: tensor([[0.3548],\n",
            "        [0.3662],\n",
            "        [0.3776],\n",
            "        [0.3890],\n",
            "        [0.4004],\n",
            "        [0.4118],\n",
            "        [0.4232],\n",
            "        [0.4346],\n",
            "        [0.4460],\n",
            "        [0.4574]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3538],\n",
            "        [0.3652],\n",
            "        [0.3767],\n",
            "        [0.3881],\n",
            "        [0.3995],\n",
            "        [0.4110],\n",
            "        [0.4224],\n",
            "        [0.4338],\n",
            "        [0.4452],\n",
            "        [0.4567]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.025829419493675232\n",
            "\n",
            "Epoch: 82 | Loss: 0.025829419493675232\n",
            "Model weights: tensor([0.5733])\n",
            "Model bias: tensor([0.3533])\n",
            "Prediction before backprop: tensor([[0.3538],\n",
            "        [0.3652],\n",
            "        [0.3767],\n",
            "        [0.3881],\n",
            "        [0.3995],\n",
            "        [0.4110],\n",
            "        [0.4224],\n",
            "        [0.4338],\n",
            "        [0.4452],\n",
            "        [0.4567]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3533],\n",
            "        [0.3648],\n",
            "        [0.3762],\n",
            "        [0.3877],\n",
            "        [0.3992],\n",
            "        [0.4106],\n",
            "        [0.4221],\n",
            "        [0.4336],\n",
            "        [0.4450],\n",
            "        [0.4565]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.02548416517674923\n",
            "\n",
            "Epoch: 83 | Loss: 0.02548416517674923\n",
            "Model weights: tensor([0.5748])\n",
            "Model bias: tensor([0.3523])\n",
            "Prediction before backprop: tensor([[0.3533],\n",
            "        [0.3648],\n",
            "        [0.3762],\n",
            "        [0.3877],\n",
            "        [0.3992],\n",
            "        [0.4106],\n",
            "        [0.4221],\n",
            "        [0.4336],\n",
            "        [0.4450],\n",
            "        [0.4565]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3523],\n",
            "        [0.3638],\n",
            "        [0.3753],\n",
            "        [0.3868],\n",
            "        [0.3983],\n",
            "        [0.4098],\n",
            "        [0.4213],\n",
            "        [0.4328],\n",
            "        [0.4443],\n",
            "        [0.4558]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.025144213810563087\n",
            "\n",
            "Epoch: 84 | Loss: 0.025144213810563087\n",
            "Model weights: tensor([0.5766])\n",
            "Model bias: tensor([0.3518])\n",
            "Prediction before backprop: tensor([[0.3523],\n",
            "        [0.3638],\n",
            "        [0.3753],\n",
            "        [0.3868],\n",
            "        [0.3983],\n",
            "        [0.4098],\n",
            "        [0.4213],\n",
            "        [0.4328],\n",
            "        [0.4443],\n",
            "        [0.4558]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3518],\n",
            "        [0.3633],\n",
            "        [0.3749],\n",
            "        [0.3864],\n",
            "        [0.3979],\n",
            "        [0.4095],\n",
            "        [0.4210],\n",
            "        [0.4325],\n",
            "        [0.4441],\n",
            "        [0.4556]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.02479521557688713\n",
            "\n",
            "Epoch: 85 | Loss: 0.02479521557688713\n",
            "Model weights: tensor([0.5784])\n",
            "Model bias: tensor([0.3513])\n",
            "Prediction before backprop: tensor([[0.3518],\n",
            "        [0.3633],\n",
            "        [0.3749],\n",
            "        [0.3864],\n",
            "        [0.3979],\n",
            "        [0.4095],\n",
            "        [0.4210],\n",
            "        [0.4325],\n",
            "        [0.4441],\n",
            "        [0.4556]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3513],\n",
            "        [0.3629],\n",
            "        [0.3744],\n",
            "        [0.3860],\n",
            "        [0.3976],\n",
            "        [0.4092],\n",
            "        [0.4207],\n",
            "        [0.4323],\n",
            "        [0.4439],\n",
            "        [0.4554]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.024458957836031914\n",
            "\n",
            "Epoch: 86 | Loss: 0.024458957836031914\n",
            "Model weights: tensor([0.5800])\n",
            "Model bias: tensor([0.3503])\n",
            "Prediction before backprop: tensor([[0.3513],\n",
            "        [0.3629],\n",
            "        [0.3744],\n",
            "        [0.3860],\n",
            "        [0.3976],\n",
            "        [0.4092],\n",
            "        [0.4207],\n",
            "        [0.4323],\n",
            "        [0.4439],\n",
            "        [0.4554]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3503],\n",
            "        [0.3619],\n",
            "        [0.3735],\n",
            "        [0.3851],\n",
            "        [0.3967],\n",
            "        [0.4083],\n",
            "        [0.4199],\n",
            "        [0.4315],\n",
            "        [0.4431],\n",
            "        [0.4547]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.024110013619065285\n",
            "\n",
            "Epoch: 87 | Loss: 0.024110013619065285\n",
            "Model weights: tensor([0.5818])\n",
            "Model bias: tensor([0.3498])\n",
            "Prediction before backprop: tensor([[0.3503],\n",
            "        [0.3619],\n",
            "        [0.3735],\n",
            "        [0.3851],\n",
            "        [0.3967],\n",
            "        [0.4083],\n",
            "        [0.4199],\n",
            "        [0.4315],\n",
            "        [0.4431],\n",
            "        [0.4547]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3498],\n",
            "        [0.3614],\n",
            "        [0.3731],\n",
            "        [0.3847],\n",
            "        [0.3964],\n",
            "        [0.4080],\n",
            "        [0.4196],\n",
            "        [0.4313],\n",
            "        [0.4429],\n",
            "        [0.4545]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.02376994863152504\n",
            "\n",
            "Epoch: 88 | Loss: 0.02376994863152504\n",
            "Model weights: tensor([0.5834])\n",
            "Model bias: tensor([0.3488])\n",
            "Prediction before backprop: tensor([[0.3498],\n",
            "        [0.3614],\n",
            "        [0.3731],\n",
            "        [0.3847],\n",
            "        [0.3964],\n",
            "        [0.4080],\n",
            "        [0.4196],\n",
            "        [0.4313],\n",
            "        [0.4429],\n",
            "        [0.4545]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3488],\n",
            "        [0.3605],\n",
            "        [0.3721],\n",
            "        [0.3838],\n",
            "        [0.3955],\n",
            "        [0.4072],\n",
            "        [0.4188],\n",
            "        [0.4305],\n",
            "        [0.4422],\n",
            "        [0.4538]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.02342480979859829\n",
            "\n",
            "Epoch: 89 | Loss: 0.02342480979859829\n",
            "Model weights: tensor([0.5852])\n",
            "Model bias: tensor([0.3483])\n",
            "Prediction before backprop: tensor([[0.3488],\n",
            "        [0.3605],\n",
            "        [0.3721],\n",
            "        [0.3838],\n",
            "        [0.3955],\n",
            "        [0.4072],\n",
            "        [0.4188],\n",
            "        [0.4305],\n",
            "        [0.4422],\n",
            "        [0.4538]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3483],\n",
            "        [0.3600],\n",
            "        [0.3717],\n",
            "        [0.3834],\n",
            "        [0.3951],\n",
            "        [0.4068],\n",
            "        [0.4185],\n",
            "        [0.4302],\n",
            "        [0.4419],\n",
            "        [0.4536]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.023080935701727867\n",
            "\n",
            "Epoch: 90 | Loss: 0.023080935701727867\n",
            "Model weights: tensor([0.5868])\n",
            "Model bias: tensor([0.3473])\n",
            "Prediction before backprop: tensor([[0.3483],\n",
            "        [0.3600],\n",
            "        [0.3717],\n",
            "        [0.3834],\n",
            "        [0.3951],\n",
            "        [0.4068],\n",
            "        [0.4185],\n",
            "        [0.4302],\n",
            "        [0.4419],\n",
            "        [0.4536]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3473],\n",
            "        [0.3590],\n",
            "        [0.3708],\n",
            "        [0.3825],\n",
            "        [0.3943],\n",
            "        [0.4060],\n",
            "        [0.4177],\n",
            "        [0.4295],\n",
            "        [0.4412],\n",
            "        [0.4529]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.022739607840776443\n",
            "\n",
            "Epoch: 91 | Loss: 0.022739607840776443\n",
            "Model weights: tensor([0.5886])\n",
            "Model bias: tensor([0.3468])\n",
            "Prediction before backprop: tensor([[0.3473],\n",
            "        [0.3590],\n",
            "        [0.3708],\n",
            "        [0.3825],\n",
            "        [0.3943],\n",
            "        [0.4060],\n",
            "        [0.4177],\n",
            "        [0.4295],\n",
            "        [0.4412],\n",
            "        [0.4529]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3468],\n",
            "        [0.3586],\n",
            "        [0.3704],\n",
            "        [0.3821],\n",
            "        [0.3939],\n",
            "        [0.4057],\n",
            "        [0.4174],\n",
            "        [0.4292],\n",
            "        [0.4410],\n",
            "        [0.4528]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.022391926497220993\n",
            "\n",
            "Epoch: 92 | Loss: 0.022391926497220993\n",
            "Model weights: tensor([0.5902])\n",
            "Model bias: tensor([0.3458])\n",
            "Prediction before backprop: tensor([[0.3468],\n",
            "        [0.3586],\n",
            "        [0.3704],\n",
            "        [0.3821],\n",
            "        [0.3939],\n",
            "        [0.4057],\n",
            "        [0.4174],\n",
            "        [0.4292],\n",
            "        [0.4410],\n",
            "        [0.4528]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3458],\n",
            "        [0.3576],\n",
            "        [0.3694],\n",
            "        [0.3812],\n",
            "        [0.3930],\n",
            "        [0.4048],\n",
            "        [0.4166],\n",
            "        [0.4284],\n",
            "        [0.4402],\n",
            "        [0.4520]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.022054409608244896\n",
            "\n",
            "Epoch: 93 | Loss: 0.022054409608244896\n",
            "Model weights: tensor([0.5920])\n",
            "Model bias: tensor([0.3453])\n",
            "Prediction before backprop: tensor([[0.3458],\n",
            "        [0.3576],\n",
            "        [0.3694],\n",
            "        [0.3812],\n",
            "        [0.3930],\n",
            "        [0.4048],\n",
            "        [0.4166],\n",
            "        [0.4284],\n",
            "        [0.4402],\n",
            "        [0.4520]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3453],\n",
            "        [0.3571],\n",
            "        [0.3690],\n",
            "        [0.3808],\n",
            "        [0.3927],\n",
            "        [0.4045],\n",
            "        [0.4164],\n",
            "        [0.4282],\n",
            "        [0.4400],\n",
            "        [0.4519]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.02170540764927864\n",
            "\n",
            "Epoch: 94 | Loss: 0.02170540764927864\n",
            "Model weights: tensor([0.5938])\n",
            "Model bias: tensor([0.3448])\n",
            "Prediction before backprop: tensor([[0.3453],\n",
            "        [0.3571],\n",
            "        [0.3690],\n",
            "        [0.3808],\n",
            "        [0.3927],\n",
            "        [0.4045],\n",
            "        [0.4164],\n",
            "        [0.4282],\n",
            "        [0.4400],\n",
            "        [0.4519]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3448],\n",
            "        [0.3567],\n",
            "        [0.3686],\n",
            "        [0.3804],\n",
            "        [0.3923],\n",
            "        [0.4042],\n",
            "        [0.4161],\n",
            "        [0.4279],\n",
            "        [0.4398],\n",
            "        [0.4517]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.021366719156503677\n",
            "\n",
            "Epoch: 95 | Loss: 0.021366719156503677\n",
            "Model weights: tensor([0.5954])\n",
            "Model bias: tensor([0.3438])\n",
            "Prediction before backprop: tensor([[0.3448],\n",
            "        [0.3567],\n",
            "        [0.3686],\n",
            "        [0.3804],\n",
            "        [0.3923],\n",
            "        [0.4042],\n",
            "        [0.4161],\n",
            "        [0.4279],\n",
            "        [0.4398],\n",
            "        [0.4517]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3438],\n",
            "        [0.3557],\n",
            "        [0.3676],\n",
            "        [0.3795],\n",
            "        [0.3914],\n",
            "        [0.4033],\n",
            "        [0.4153],\n",
            "        [0.4272],\n",
            "        [0.4391],\n",
            "        [0.4510]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.021020207554101944\n",
            "\n",
            "Epoch: 96 | Loss: 0.021020207554101944\n",
            "Model weights: tensor([0.5972])\n",
            "Model bias: tensor([0.3433])\n",
            "Prediction before backprop: tensor([[0.3438],\n",
            "        [0.3557],\n",
            "        [0.3676],\n",
            "        [0.3795],\n",
            "        [0.3914],\n",
            "        [0.4033],\n",
            "        [0.4153],\n",
            "        [0.4272],\n",
            "        [0.4391],\n",
            "        [0.4510]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3433],\n",
            "        [0.3553],\n",
            "        [0.3672],\n",
            "        [0.3791],\n",
            "        [0.3911],\n",
            "        [0.4030],\n",
            "        [0.4150],\n",
            "        [0.4269],\n",
            "        [0.4389],\n",
            "        [0.4508]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.020677709951996803\n",
            "\n",
            "Epoch: 97 | Loss: 0.020677709951996803\n",
            "Model weights: tensor([0.5988])\n",
            "Model bias: tensor([0.3423])\n",
            "Prediction before backprop: tensor([[0.3433],\n",
            "        [0.3553],\n",
            "        [0.3672],\n",
            "        [0.3791],\n",
            "        [0.3911],\n",
            "        [0.4030],\n",
            "        [0.4150],\n",
            "        [0.4269],\n",
            "        [0.4389],\n",
            "        [0.4508]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3423],\n",
            "        [0.3543],\n",
            "        [0.3663],\n",
            "        [0.3782],\n",
            "        [0.3902],\n",
            "        [0.4022],\n",
            "        [0.4142],\n",
            "        [0.4261],\n",
            "        [0.4381],\n",
            "        [0.4501]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.02033500373363495\n",
            "\n",
            "Epoch: 98 | Loss: 0.02033500373363495\n",
            "Model weights: tensor([0.6006])\n",
            "Model bias: tensor([0.3418])\n",
            "Prediction before backprop: tensor([[0.3423],\n",
            "        [0.3543],\n",
            "        [0.3663],\n",
            "        [0.3782],\n",
            "        [0.3902],\n",
            "        [0.4022],\n",
            "        [0.4142],\n",
            "        [0.4261],\n",
            "        [0.4381],\n",
            "        [0.4501]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3418],\n",
            "        [0.3538],\n",
            "        [0.3658],\n",
            "        [0.3778],\n",
            "        [0.3899],\n",
            "        [0.4019],\n",
            "        [0.4139],\n",
            "        [0.4259],\n",
            "        [0.4379],\n",
            "        [0.4499]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n",
            "Loss: 0.01998869702219963\n",
            "\n",
            "Epoch: 99 | Loss: 0.01998869702219963\n",
            "Model weights: tensor([0.6022])\n",
            "Model bias: tensor([0.3408])\n",
            "Prediction before backprop: tensor([[0.3418],\n",
            "        [0.3538],\n",
            "        [0.3658],\n",
            "        [0.3778],\n",
            "        [0.3899],\n",
            "        [0.4019],\n",
            "        [0.4139],\n",
            "        [0.4259],\n",
            "        [0.4379],\n",
            "        [0.4499]], grad_fn=<SliceBackward0>)\n",
            "Ground truth: tensor([[0.3000],\n",
            "        [0.3140],\n",
            "        [0.3280],\n",
            "        [0.3420],\n",
            "        [0.3560],\n",
            "        [0.3700],\n",
            "        [0.3840],\n",
            "        [0.3980],\n",
            "        [0.4120],\n",
            "        [0.4260]])\n",
            "Prediction after backprop: tensor([[0.3408],\n",
            "        [0.3529],\n",
            "        [0.3649],\n",
            "        [0.3769],\n",
            "        [0.3890],\n",
            "        [0.4010],\n",
            "        [0.4131],\n",
            "        [0.4251],\n",
            "        [0.4372],\n",
            "        [0.4492]], grad_fn=<SliceBackward0>)\n",
            "Model weights grad: None\n",
            "Model bias grad: None\n",
            "--------------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "id": "VWFIJvXS075x",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "266edf69-80ad-413d-e62a-bacba5a51853"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3445])), ('bias', tensor([0.1488]))])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "weight, bias"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aoUkQVqj5bdw",
        "outputId": "5d906012-8ded-45e2-f0c0-d4455858e0de"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.7, 0.3)"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_0.state_dict()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aqWw4BgQ9ZL-",
        "outputId": "7fc6a90c-4eba-440e-8d67-aa6e0cb72e10"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "OrderedDict([('weights', tensor([0.3406])), ('bias', tensor([0.1388]))])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.inference_mode():\n",
        "  y_preds = model_0(X_test)\n",
        "\n",
        "plot_predictions(predictions=y_preds)"
      ],
      "metadata": {
        "id": "S8FmEZWJ90zb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 599
        },
        "outputId": "b089d982-677f-45f1-80ef-24fd4e91878e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x700 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vsFBl9C1guLW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}